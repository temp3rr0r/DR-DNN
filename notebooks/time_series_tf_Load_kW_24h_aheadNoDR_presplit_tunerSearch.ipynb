{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Decomposition-Residuals Neural Networks: Hybrid system identification applied to electricity forecasting\n",
    "\n",
    "### Konstantinos Theodorakos\n",
    "##### Ph.D. student\n",
    "#### KU Leuven, Belgium\n",
    "konstantinos.theodorakos@esat.kuleuven.be\n",
    "##### Dept. of Electrical Engineering (ESAT) – Research Group STADIUS, Center for Dynamical Systems, Signal Processing, and Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Code adapted from: https://www.tensorflow.org/tutorials/structured_data/time_series\n",
    "\n",
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/structured_data/time_series\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/time_series.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/time_series.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/structured_data/time_series.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This tutorial is an introduction to time series forecasting using TensorFlow. It builds a few different styles of models including Convolutional and Recurrent Neural Networks (CNNs and RNNs).\n",
    "\n",
    "This is covered in two main parts, with subsections:\n",
    "\n",
    "* Forecast for a single timestep:\n",
    "  * A single feature.\n",
    "  * All features.\n",
    "* Forecast multiple steps:\n",
    "  * Single-shot: Make the predictions all at once.\n",
    "  * Autoregressive: Make one prediction at a time and feed the output back to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'math' from 'scipy' (c:\\Users\\madks\\.conda\\envs\\tensorflow\\lib\\site-packages\\scipy\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3bf729fb3690>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../customTensorflowLayers/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mDataframeDecompositionLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\madks\\Documents\\GitHub\\smartMeters\\customTensorflowLayers\\DataframeDecompositionLayer.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0macoustics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcepstrum\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreal_cepstrum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\madks\\.conda\\envs\\tensorflow\\lib\\site-packages\\acoustics\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \"\"\"\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0macoustics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0macoustics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mambisonics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0macoustics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matmosphere\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0macoustics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbands\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\madks\\.conda\\envs\\tensorflow\\lib\\site-packages\\acoustics\\ambisonics.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mfactorial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactorial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'math' from 'scipy' (c:\\Users\\madks\\.conda\\envs\\tensorflow\\lib\\site-packages\\scipy\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('../customTensorflowLayers/')\n",
    "import DataframeDecompositionLayer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "do_DecompositionResiduals = False # DR-DNN or plain DNN\n",
    "cpu_only = False # CPU or GPU acceleration\n",
    "do_extras = False # Extras: 3 STD outliers, Day/year sines,\n",
    "use_real_cepstrum = False\n",
    "MAX_EPOCHS = 20 # 150 # 20\n",
    "stop_early_patience = 5\n",
    "window_stride = 1 # 12 # 24 # 1 # hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if cpu_only:\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # These lines should be called asap, after the os import\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Use CPU only by default\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv(csv_path)\n",
    "# df = pd.read_excel(\"../../next-dayCOVID-19/data/Actuals.xlsx\") # Read from excel\n",
    "# df = pd.read_excel(\"../../next-dayCOVID-19/data/ActualsAll.xlsx\") # Read from excel\n",
    "df = pd.read_excel(\"../../next-dayCOVID-19/data/ActualsAll_8PM.xlsx\") # Read from excel\n",
    "# df = pd.read_excel(\"../../next-dayCOVID-19/data/ActualsAll_8PM_weather_forecast_48h.xlsx\") # Read from excel\n",
    "\n",
    "# date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
    "df.index = pd.to_datetime(df.pop('Time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Pre/Post-COVID: first 2.5 years vs last 1 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df = df.head(int(365 * 24 * 2.5)).copy() # Pre-COVID: first 2.5 years\n",
    "# df = df.tail(int(365 * 24 * 2)).copy() # Post-COVID: last 2 years\n",
    "# df = df.tail(24 * 30)  # A few samples for quick testing\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Calculate data split indices from pre-calculation of DR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We'll use a `(70%, 20%, 10%)` split for the training, validation, and test sets. Note the data is **not** being randomly shuffled before splitting. This is for two reasons.\n",
    "\n",
    "1. It ensures that chopping the data into windows of consecutive samples is still possible.\n",
    "2. It ensures that the validation/test results are more realistic, being evaluated on data collected after the model was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.7 # 0.98  # 0.7\n",
    "validation_ratio = 0.2 # 0.01  # 0.2\n",
    "test_ratio = 1 - (train_ratio + validation_ratio)\n",
    "print(\"Train/validation/test: {}%/{}%/{}%\".format(np.round(train_ratio * 100, 3), np.round(validation_ratio * 100, 3), np.round(test_ratio * 100, 3)))\n",
    "\n",
    "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "\n",
    "n = len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# min_z = 3\n",
    "# z = np.abs(stats.zscore(df))\n",
    "# print(df.shape)\n",
    "# print(np.where(z > min_z))\n",
    "#\n",
    "# df_clean = df.copy()\n",
    "# df_clean.loc[(z > min_z).all(axis=1)] = np.nan\n",
    "# df_clean = df_clean.interpolate(\"time\").bfill().ffill()\n",
    "# print(df_clean.shape)\n",
    "# df = df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_DR_df(time_series):\n",
    "    layer_decomposition = DataframeDecompositionLayer(\n",
    "        forecast_STL=False, freq=\"H\", period=24, return_dataframe=True,\n",
    "        endogenous_variables=[\"Load (kW)\",\n",
    "                              # \"Temperature (C) \"\n",
    "                              ],\n",
    "        multiple_lags=True,\n",
    "        use_real_cepstrum=use_real_cepstrum,\n",
    "    )\n",
    "    out_layer_decomposition = layer_decomposition(\n",
    "        time_series, forecast_STL=False, freq=\"H\", period=24, return_dataframe=True,\n",
    "        endogenous_variables=[\"Load (kW)\",\n",
    "                              # \"Temperature (C) \"\n",
    "                              ],\n",
    "        multiple_lags=True,\n",
    "        use_real_cepstrum=use_real_cepstrum,\n",
    "    )\n",
    "\n",
    "    return out_layer_decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# TODO: Split preprocessing\n",
    "\n",
    "## Different on each subset:\n",
    "1. **train**\n",
    "2. **train+val**\n",
    "3. **train+val+test** sequence...\n",
    "\n",
    "### From the START:\n",
    "SPLIT data: train, train+val, train+val+test sequence...\n",
    "\n",
    "#### Then for each subset:\n",
    "\n",
    "1. Remove 3 STD outliers (optional, affects post-sample).\n",
    "2. Apply DR (affects post-sample).\n",
    "3. Feature engineering wind (rolling: not affected post-sample).\n",
    "4. Add weekly lag (rolling: not affected in-sample).\n",
    "5. Normalize (on train mean/std) (affects post-sample).\n",
    "\n",
    "## TODO: IF 7-day lag baseline can't be beaten...\n",
    "\n",
    "10-iter hyperopt x 10 experiment runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_type = \"No DR\"\n",
    "if do_DecompositionResiduals:\n",
    "\n",
    "    model_type = \"DR-DNN\"\n",
    "\n",
    "    # DR split\n",
    "    pre_train_df = df[0:int(n*train_ratio)].copy()\n",
    "    pre_train_val_df = df[0:int(n*(train_ratio + validation_ratio))].copy()\n",
    "    pre_train_val_test_df = df.copy()\n",
    "\n",
    "    # Gradual DR fit\n",
    "    dr_train = get_DR_df(pre_train_df)\n",
    "    dr_train_val = get_DR_df(pre_train_val_df)\n",
    "    dr_train_val_test = get_DR_df(pre_train_val_test_df)\n",
    "\n",
    "    # Replace DR segments of gradual fit: train/val/test\n",
    "    dr_train_val_test[0:int(n*train_ratio)] = dr_train[0:int(n*train_ratio)]  # Replace with old train decomp\n",
    "    dr_train_val_test[int(n*train_ratio):int(n*(train_ratio + validation_ratio))] = dr_train_val[int(n*train_ratio):int(n*(train_ratio + validation_ratio))]  # Replace with old val decomp. Test decomp not needed\n",
    "    df = dr_train_val_test.copy()\n",
    "\n",
    "date_time = df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's take a glance at the data. Here are the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here is the evolution of a few features over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot_cols = ['T (degC)', 'p (mbar)', 'rho (g/m**3)']\n",
    "plot_cols = ['Load (kW)', 'Pressure_kpa', 'Cloud Cover (%)', 'Humidity (%)',\n",
    "       'Temperature (C) ', 'Wind Direction (deg)', 'Wind Speed (kmh)']\n",
    "\n",
    "plot_features = df[plot_cols]\n",
    "plot_features.index = date_time\n",
    "_ = plot_features.plot(subplots=True)\n",
    "\n",
    "plot_features = df[plot_cols][:480]\n",
    "plot_features.index = date_time[:480]\n",
    "_ = plot_features.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Inspect and cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next look at the statistics of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Wind velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "One thing that should stand out is the `min` value of the wind velocity, `wv (m/s)` and `max. wv (m/s)` columns. This `-9999` is likely erroneous. There's a separate wind direction column, so the velocity should be `>=0`. Replace it with zeros:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# wv = df['wv (m/s)']\n",
    "# bad_wv = wv == -9999.0\n",
    "# wv[bad_wv] = 0.0\n",
    "#\n",
    "# max_wv = df['max. wv (m/s)']\n",
    "# bad_max_wv = max_wv == -9999.0\n",
    "# max_wv[bad_max_wv] = 0.0\n",
    "#\n",
    "# # The above inplace edits are reflected in the DataFrame\n",
    "# df['wv (m/s)'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature engineering\n",
    "\n",
    "Before diving in to build a model it's important to understand your data, and be sure that you're passing the model appropriately formatted data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Wind\n",
    "The last column of the data, `wd (deg)`, gives the wind direction in units of degrees. Angles do not make good model inputs, 360° and 0° should be close to each other, and wrap around smoothly. Direction shouldn't matter if the wind is not blowing.\n",
    "\n",
    "Right now the distribution of wind data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.axis('tight')\n",
    "plt.hist2d(df['Wind Direction (deg)'], df['Wind Speed (kmh)'],\n",
    "           bins=(50, 50),\n",
    "           vmax=400, # linear normalization\n",
    "           # range=[[0, 360], [0, 60]],\n",
    "           # cmap=plt.get_cmap(\"jet\"), # turbo, jet, rainbow\n",
    "           )\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xlabel('Wind Direction [deg]')\n",
    "plt.ylabel('Wind Velocity [km/h]')\n",
    "plt.savefig(\"wind_direction_velocity.svg\")\n",
    "plt.savefig(\"wind_direction_velocity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "But this will be easier for the model to interpret if you convert the wind direction and velocity columns to a wind **vector**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wv = df.pop('Wind Speed (kmh)')\n",
    "# max_wv = df.pop('max. wv (m/s)')\n",
    "\n",
    "# Convert to radians.\n",
    "wd_rad = df.pop('Wind Direction (deg)') * np.pi / 180.0\n",
    "\n",
    "# Calculate the wind x and y components.\n",
    "df['Wx'] = wv * np.cos(wd_rad)\n",
    "df['Wy'] = wv * np.sin(wd_rad)\n",
    "\n",
    "# # Calculate the max wind x and y components.\n",
    "# df['max Wx'] = max_wv*np.cos(wd_rad)\n",
    "# df['max Wy'] = max_wv*np.sin(wd_rad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The distribution of wind vectors is much simpler for the model to correctly interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.axis('tight')\n",
    "plt.hist2d(df['Wx'], df['Wy'],\n",
    "              bins=(50, 50),\n",
    "           # range=[[-60, 60], [-60, 60]],\n",
    "           # cmap=plt.get_cmap(\"jet\"), # turbo, jet, rainbow\n",
    "           vmax=400 # linear normalization\n",
    "           )\n",
    "plt.colorbar()\n",
    "plt.xlabel('Wind X [km/h]')\n",
    "plt.ylabel('Wind Y [km/h]')\n",
    "\n",
    "plt.savefig(\"wind_vectors.svg\")\n",
    "plt.savefig(\"wind_vectors.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Similarly the `Date Time` column is very useful, but not in this string form. Start by converting it to seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "timestamp_s = date_time.map(datetime.datetime.timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Similar to the wind direction the time in seconds is not a useful model input. Being weather data it has clear daily and yearly periodicity. There are many ways you could deal with periodicity.\n",
    "\n",
    "A simple approach to convert it to a usable signal is to use `sin` and `cos` to convert the time to clear \"Time of day\" and \"Time of year\" signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if do_DecompositionResiduals and do_extras:\n",
    "    day = 24*60*60\n",
    "    year = (365.2425)*day\n",
    "\n",
    "    df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "    df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "    df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "    df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if do_DecompositionResiduals:\n",
    "    plot_range = 24 * 7\n",
    "\n",
    "    legend = []\n",
    "    for column in df.columns:\n",
    "        if \"cos\" in column or \"sin\" in column or \"time\" in column:\n",
    "            plt.plot(np.array(df[column])[:plot_range])\n",
    "            legend.append(column)\n",
    "    plt.legend(legend)\n",
    "    plt.xlabel('Time [h]')\n",
    "    plt.title('Calendar signals (week)')\n",
    "    plt.savefig(\"calendar_DR.svg\")\n",
    "    plt.savefig(\"calendar_DR.png\")\n",
    "    plt.savefig(\"calendar_DR.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This gives the model access to the most important frequency features. In this case you knew ahead of time which frequencies were important.\n",
    "\n",
    "If you didn't know, you can determine which frequencies are important using an `fft`. To check our assumptions, here is the `tf.signal.rfft` of the temperature over time. Note the obvious peaks at frequencies near `1/year` and `1/day`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if do_DecompositionResiduals and do_extras:\n",
    "    fft = tf.signal.rfft(df['Load (kW)'])\n",
    "    f_per_dataset = np.arange(0, len(fft))\n",
    "\n",
    "    n_samples_h = len(df['Load (kW)'])\n",
    "    hours_per_year = 24*365.2524\n",
    "    years_per_dataset = n_samples_h/(hours_per_year)\n",
    "\n",
    "    f_per_year = f_per_dataset/years_per_dataset\n",
    "    plt.step(f_per_year, np.abs(fft))\n",
    "    plt.xscale('log')\n",
    "    plt.ylim(0, 2.5 * 1e9)\n",
    "    plt.xlim([0.1, max(plt.xlim())])\n",
    "    plt.xticks([1, 7, 30, 365.2524/4.0, 365.2524], labels=['1/Year', '1/Quarter-year', '1/month', '1/week', '1/day'])\n",
    "    _ = plt.xlabel('Frequency (log scale)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Weekly lag\n",
    "Repeat signal from exactly 24 hours x 6/7/8 days ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['Load (kW)_lag7d'] = df['Load (kW)'].shift(24*7).bfill()\n",
    "df['Load (kW)_lag6d'] = df['Load (kW)'].shift(24*6).bfill()\n",
    "df['Load (kW)_lag5d'] = df['Load (kW)'].shift(24*5).bfill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "####  Do Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df = df[0:int(n*train_ratio)]\n",
    "val_df = df[int(n*train_ratio):int(n*(train_ratio + validation_ratio))]\n",
    "test_df = df[int(n*(train_ratio + validation_ratio)):]\n",
    "\n",
    "num_features = df.shape[1]\n",
    "\n",
    "# Plot time periods:\n",
    "print(\"Train (\", train_df.shape[0], \"):\\n\\t\", train_df.head(1).index.values, \" to \", train_df.tail(1).index.values)\n",
    "print(\"Validation (\", val_df.shape[0], \"):\\n\\t\", val_df.head(1).index.values, \" to \", val_df.tail(1).index.values)\n",
    "print(\"Test (\", test_df.shape[0], \"):\\n\\t\", test_df.head(1).index.values, \" to \", test_df.tail(1).index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Normalize the data\n",
    "\n",
    "It is important to scale features before training a neural network. Normalization is a common way of doing this scaling. Subtract the mean and divide by the standard deviation of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The mean and standard deviation should only be computed using the training data so that the models have no access to the values in the validation and test sets.\n",
    "\n",
    "It's also arguable that the model shouldn't have access to future values in the training set when training, and that this normalization should be done using moving averages. That's not the focus of this tutorial, and the validation and test sets ensure that you get (somewhat) honest metrics. So in the interest of simplicity this tutorial uses a simple average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "val_df = (val_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now peek at the distribution of the features. Some features do have long tails, but there are no obvious errors like the `-9999` wind velocity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_std = (df - train_mean) / train_std\n",
    "df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n",
    "_ = ax.set_xticklabels(df.keys(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data windowing\n",
    "\n",
    "The models in this tutorial will make a set of predictions based on a window of consecutive samples from the data.\n",
    "\n",
    "The main features of the input windows are:\n",
    "\n",
    "* The width (number of time steps) of the input and label windows\n",
    "* The time offset between them.\n",
    "* Which features are used as inputs, labels, or both.\n",
    "\n",
    "This tutorial builds a variety of models (including Linear, DNN, CNN and RNN models), and uses them for both:\n",
    "\n",
    "* *Single-output*, and *multi-output* predictions.\n",
    "* *Single-time-step* and *multi-time-step* predictions.\n",
    "\n",
    "This section focuses on implementing the data windowing so that it can be reused for all of those models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Depending on the task and type of model you may want to generate a variety of data windows. Here are some examples:\n",
    "\n",
    "1. For example, to make a single prediction 24h into the future, given 24h of history you might define a window like this:\n",
    "\n",
    "  ![One prediction 24h into the future.](images/raw_window_24h.png)\n",
    "\n",
    "2. A model that makes a prediction 1h into the future, given 6h of history would need a window like this:\n",
    "\n",
    "  ![One prediction 1h into the future.](images/raw_window_1h.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The rest of this section defines a `WindowGenerator` class. This class can:\n",
    "\n",
    "1. Handle the indexes and offsets as shown in the diagrams above.\n",
    "1. Split windows of features into a `(features, labels)` pairs.\n",
    "2. Plot the content of the resulting windows.\n",
    "3. Efficiently generate batches of these windows from the training, evaluation, and test data, using `tf.data.Dataset`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Indexes and offsets\n",
    "\n",
    "Start by creating the `WindowGenerator` class. The `__init__` method includes all the necessary logic for the input and label indices.\n",
    "\n",
    "It also takes the train, eval, and test dataframes as input. These will be converted to `tf.data.Dataset`s of windows later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "               label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(train_df.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here is code to create the 2 windows shown in the diagrams at the start of this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w1 = WindowGenerator(input_width=24, label_width=1, shift=24,\n",
    "                     label_columns=['Load (kW)'])\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w2 = WindowGenerator(input_width=6, label_width=1, shift=1,\n",
    "                     label_columns=['Load (kW)'])\n",
    "w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. Split\n",
    "Given a list consecutive inputs, the `split_window` method will convert them to a window of inputs and a window of labels.\n",
    "\n",
    "The example `w2`, above, will be split like this:\n",
    "\n",
    "![The initial window is all consecutive samples, this splits it into an (inputs, labels) pairs](images/split_window.png)\n",
    "\n",
    "This diagram doesn't show the `features` axis of the data, but this `split_window` function also handles the `label_columns` so it can be used for both the single output and multi-output examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "  inputs = features[:, self.input_slice, :]\n",
    "  labels = features[:, self.labels_slice, :]\n",
    "  if self.label_columns is not None:\n",
    "    labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "  # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  inputs.set_shape([None, self.input_width, None])\n",
    "  labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Stack three slices, the length of the total window:\n",
    "example_window = tf.stack([np.array(train_df[:w2.total_window_size]),\n",
    "                           np.array(train_df[100:100+w2.total_window_size]),\n",
    "                           np.array(train_df[200:200+w2.total_window_size])])\n",
    "\n",
    "\n",
    "example_inputs, example_labels = w2.split_window(example_window)\n",
    "\n",
    "print('All shapes are: (batch, time, features)')\n",
    "print(f'Window shape: {example_window.shape}')\n",
    "print(f'Inputs shape: {example_inputs.shape}')\n",
    "print(f'labels shape: {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Typically data in TensorFlow is packed into arrays where the outermost index is across examples (the \"batch\" dimension). The middle indices are the \"time\" or \"space\" (width, height) dimension(s). The innermost indices are the features.\n",
    "\n",
    "The code above took a batch of 3, 7-timestep windows, with 19 features at each time step. It split them into a batch of 6-timestep, 19 feature inputs, and a 1-timestep 1-feature label. The label only has one feature because the `WindowGenerator` was initialized with `label_columns=['T (degC)']`. Initially this tutorial will build models that predict single output labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3. Plot\n",
    "\n",
    "Here is a plot method that allows a simple visualization of the split window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w2.example = example_inputs, example_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot(self, model=None, plot_col='Load (kW)', model_title=\"DR-DNN\", max_subplots=1):\n",
    "  inputs, labels = self.example\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plot_col_index = self.column_indices[plot_col]\n",
    "  max_n = min(max_subplots, len(inputs))\n",
    "  for n in range(max_n):\n",
    "    plt.subplot(3, 1, n+1)\n",
    "    plt.ylabel(f'{plot_col} [normed]')\n",
    "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "    if self.label_columns:\n",
    "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "    else:\n",
    "      label_col_index = plot_col_index\n",
    "\n",
    "    if label_col_index is None:\n",
    "      continue\n",
    "\n",
    "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                edgecolors='k', label='Measurements', c='#2ca02c', s=64)\n",
    "    if model is not None:\n",
    "      predictions = model(inputs)\n",
    "      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "\n",
    "    if n == 0:\n",
    "      plt.legend()\n",
    "\n",
    "  plt.xlabel('Time [h]')\n",
    "  plt.title(\"{} 24h input, 24h-ahead forecast: Load (kW)\".format(model_title))\n",
    "\n",
    "WindowGenerator.plot = plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This plot aligns inputs, labels, and (later) predictions based on the time that the item refers to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w2.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can plot the other columns, but the example window `w2` configuration only has labels for the `T (degC)` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w2.plot(plot_col='Pressure_kpa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4. Create `tf.data.Dataset`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Finally this `make_dataset` method will take a time series `DataFrame` and convert it to a `tf.data.Dataset` of `(input_window, label_window)` pairs using the `preprocessing.timeseries_dataset_from_array` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_dataset(self, data):\n",
    "  data = np.array(data, dtype=np.float32)\n",
    "  ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "      data=data,\n",
    "      targets=None,\n",
    "      sequence_length=self.total_window_size,\n",
    "      sequence_stride=window_stride,\n",
    "      # shuffle=True,\n",
    "      shuffle=False,\n",
    "      batch_size=32,)\n",
    "\n",
    "  ds = ds.map(self.split_window)\n",
    "\n",
    "  return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The `WindowGenerator` object holds training, validation and test data. Add properties for accessing them as `tf.data.Datasets` using the above `make_dataset` method. Also add a standard example batch for easy access and plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@property\n",
    "def train(self):\n",
    "  return self.make_dataset(self.train_df)\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "  return self.make_dataset(self.val_df)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "  return self.make_dataset(self.test_df)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "  result = getattr(self, '_example', None)\n",
    "  if result is None:\n",
    "    # No example batch was found, so get one from the `.train` dataset\n",
    "    result = next(iter(self.train))\n",
    "    # And cache it for next time\n",
    "    self._example = result\n",
    "  return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.val = val\n",
    "WindowGenerator.test = test\n",
    "WindowGenerator.example = example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now the `WindowGenerator` object gives you access to the `tf.data.Dataset` objects, so you can easily iterate over the data.\n",
    "\n",
    "The `Dataset.element_spec` property tells you the structure, `dtypes` and shapes of the dataset elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Each element is an (inputs, label) pair\n",
    "w2.train.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Iterating over a `Dataset` yields concrete batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for example_inputs, example_labels in w2.train.take(1):\n",
    "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "  print(f'Labels shape (batch, time, features): {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Single step models\n",
    "\n",
    "The simplest model you can build on this sort of data is one that predicts a single feature's value, 1 timestep (1h) in the future based only on the current conditions.\n",
    "\n",
    "So start by building models to predict the `T (degC)` value 1h into the future.\n",
    "\n",
    "![Predict the next time step](images/narrow_window.png)\n",
    "\n",
    "Configure a `WindowGenerator` object to produce these single-step `(input, label)` pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "single_step_window = WindowGenerator(\n",
    "    input_width=1, label_width=1, shift=1,\n",
    "    label_columns=['Load (kW)'])\n",
    "single_step_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The `window` object creates `tf.data.Datasets` from the training, validation, and test sets, allowing you to easily iterate over batches of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for example_inputs, example_labels in single_step_window.train.take(1):\n",
    "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "  print(f'Labels shape (batch, time, features): {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Baseline\n",
    "\n",
    "Before building a trainable model it would be good to have a performance baseline as a point for comparison with the later more complicated models.\n",
    "\n",
    "This first task is to predict temperature 1h in the future given the current value of all features. The current values include the current temperature.\n",
    "\n",
    "So start with a model that just returns the current temperature as the prediction, predicting \"No change\". This is a reasonable baseline since temperature changes slowly. Of course, this baseline will work less well if you make a prediction further in the future.\n",
    "\n",
    "![Send the input to the output](images/baseline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Baseline(tf.keras.Model):\n",
    "  def __init__(self, label_index=None):\n",
    "    super().__init__()\n",
    "    self.label_index = label_index\n",
    "\n",
    "  def call(self, inputs):\n",
    "    if self.label_index is None:\n",
    "      return inputs\n",
    "    result = inputs[:, :, self.label_index]\n",
    "    return result[:, :, tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Instantiate and evaluate this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "baseline = Baseline(label_index=column_indices['Load (kW)'])\n",
    "\n",
    "baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "# baseline.compile(loss=tf.losses.MeanAbsoluteError(),\n",
    "                 metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "val_performance = {}\n",
    "performance = {}\n",
    "val_performance['Baseline'] = baseline.evaluate(single_step_window.val)\n",
    "performance['Baseline'] = baseline.evaluate(single_step_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "That printed some performance metrics, but those don't give you a feeling for how well the model is doing.\n",
    "\n",
    "The `WindowGenerator` has a plot method, but the plots won't be very interesting with only a single sample. So, create a wider `WindowGenerator` that generates windows 24h of consecutive inputs and labels at a time.\n",
    "\n",
    "The `wide_window` doesn't change the way the model operates. The model still makes predictions 1h into the future based on a single input time step. Here the `time` axis acts like the `batch` axis: Each prediction is made independently with no interaction between time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wide_window = WindowGenerator(\n",
    "    input_width=24, label_width=24, shift=1,\n",
    "    label_columns=['Load (kW)'])\n",
    "\n",
    "wide_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This expanded window can be passed directly to the same `baseline` model without any code changes. This is possible because the inputs and labels have the same number of timesteps, and the baseline just forwards the input to the output:\n",
    "\n",
    "  ![One prediction 1h into the future, ever hour.](images/last_window.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Input shape:', wide_window.example[0].shape)\n",
    "print('Output shape:', baseline(wide_window.example[0]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Plotting the baseline model's predictions you can see that it is simply the labels, shifted right by 1h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wide_window.plot(baseline, model_title=\"Baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the above plots of three examples the single step model is run over the course of 24h. This deserves some explanation:\n",
    "\n",
    "* The blue \"Inputs\" line shows the input temperature at each time step. The model recieves all features, this plot only shows the temperature.\n",
    "* The green \"Labels\" dots show the target prediction value. These dots are shown at the prediction time, not the input time. That is why the range of labels is shifted 1 step relative to the inputs.\n",
    "* The orange \"Predictions\" crosses are the model's prediction's for each output time step. If the model were predicting perfectly the predictions would land directly on the \"labels\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Linear model\n",
    "\n",
    "The simplest **trainable** model you can apply to this task is to insert linear transformation between the input and output. In this case the output from a time step only depends on that step:\n",
    "\n",
    "![A single step prediction](images/narrow_window.png)\n",
    "\n",
    "A `layers.Dense` with no `activation` set is a linear model. The layer only transforms the last axis of the data from `(batch, time, inputs)` to `(batch, time, units)`, it is applied independently to every item across the `batch` and `time` axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "linear = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Input shape:', single_step_window.example[0].shape)\n",
    "print('Output shape:', linear(single_step_window.example[0]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This tutorial trains many models, so package the training procedure into a function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Helpful functions for experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_denormed(data, df, model, batch_size=32):\n",
    "    absolute_error = []\n",
    "    denormed_absolute_error = []\n",
    "\n",
    "    for batch_inputs, batch_labels in data.take(int(df.shape[0]/batch_size)):\n",
    "\n",
    "        prediction = model.predict(batch_inputs)\n",
    "        residuals = tf.abs(batch_labels - prediction)\n",
    "        absolute_error += residuals.numpy().flatten().tolist()\n",
    "\n",
    "        denormed_prediction = (prediction * train_std[0]) + train_mean[0]\n",
    "        denormed_expected = (batch_labels * train_std[0]) + train_mean[0]\n",
    "        denormed_residuals = tf.abs(denormed_expected - denormed_prediction)\n",
    "        denormed_absolute_error += denormed_residuals.numpy().flatten().tolist()\n",
    "\n",
    "    return np.array(absolute_error).mean(), np.array(denormed_absolute_error).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_denormed_sMAPE(data, df, model, batch_size=32):\n",
    "    denormed_sMAPE = []\n",
    "\n",
    "    for batch_inputs, batch_labels in data.take(int(df.shape[0]/batch_size)):\n",
    "\n",
    "        prediction = model.predict(batch_inputs)\n",
    "\n",
    "        denormed_prediction = (prediction * train_std[0]) + train_mean[0]\n",
    "        denormed_expected = (batch_labels * train_std[0]) + train_mean[0]\n",
    "\n",
    "        denormed_residuals = tf.abs(denormed_expected - denormed_prediction)\n",
    "        denormed_denominator = tf.abs(denormed_expected) + tf.abs(denormed_prediction)\n",
    "\n",
    "        denormed_pre_sMAPE = 2.0 * (denormed_residuals/denormed_denominator)\n",
    "\n",
    "        denormed_sMAPE += denormed_pre_sMAPE.numpy().flatten().tolist()\n",
    "    return 100.0 * np.array(denormed_sMAPE).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_denormed_MAPE(data, df, model, batch_size=32):\n",
    "    denormed_MAPE = []\n",
    "\n",
    "    for batch_inputs, batch_labels in data.take(int(df.shape[0]/batch_size)):\n",
    "\n",
    "        prediction = model.predict(batch_inputs)\n",
    "\n",
    "        denormed_prediction = (prediction * train_std[0]) + train_mean[0]\n",
    "        denormed_expected = (batch_labels * train_std[0]) + train_mean[0]\n",
    "\n",
    "        denormed_residuals = tf.abs(denormed_expected - denormed_prediction)\n",
    "\n",
    "        denormed_pre_MAPE = (denormed_residuals/denormed_expected)\n",
    "\n",
    "        denormed_MAPE += denormed_pre_MAPE.numpy().flatten().tolist()\n",
    "\n",
    "    return 100.0 * np.array(denormed_MAPE).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_denormed_MASE(data, df, model, batch_size=32):\n",
    "    denormed_MASE = []\n",
    "\n",
    "    counter = 0\n",
    "    for batch_inputs, batch_labels in data.take(int(df.shape[0]/batch_size)):\n",
    "\n",
    "        prediction = model.predict(batch_inputs)\n",
    "\n",
    "        denormed_prediction = (prediction * train_std[0]) + train_mean[0]\n",
    "        denormed_expected = (batch_labels * train_std[0]) + train_mean[0]\n",
    "        denormed_naive_m = (batch_inputs[:, :, 0:1] * train_std[0]) + train_mean[0]\n",
    "\n",
    "        denormed_residuals = tf.abs(denormed_expected - denormed_prediction)\n",
    "        denormed_residuals_naive_m = tf.abs(denormed_expected - denormed_naive_m)\n",
    "\n",
    "        denormed_pre_MASE = (denormed_residuals/denormed_residuals_naive_m)\n",
    "\n",
    "        denormed_MASE += denormed_pre_MASE.numpy().flatten().tolist()\n",
    "\n",
    "        # expected = batch_labels\n",
    "        # naive_m = batch_inputs[:, :, 0:1]\n",
    "        # residuals = tf.abs(expected - prediction)\n",
    "        # residuals_naive_m = tf.abs(expected - naive_m)\n",
    "        # pre_MASE = (residuals/residuals_naive_m)\n",
    "        # denormed_MASE += pre_MASE.numpy().flatten().tolist()\n",
    "    return 100.0 * np.array(denormed_MASE).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_denormed_all(data, df, model, batch_size=32, forecast_horizon=24, m_interval=24):\n",
    "\n",
    "    MAE = []\n",
    "    denormed_MAE = []\n",
    "    denormed_sMAPE = []\n",
    "    denormed_MAPE = []\n",
    "    denormed_MASE = []\n",
    "\n",
    "    for batch_inputs, batch_labels in data.take(int(df.shape[0]/batch_size)):\n",
    "\n",
    "        horizon_batches = batch_inputs.shape[0] * forecast_horizon\n",
    "\n",
    "        prediction = model.predict(batch_inputs)\n",
    "\n",
    "        denormed_prediction = (prediction * train_std[0]) + train_mean[0]\n",
    "        denormed_expected = (batch_labels * train_std[0]) + train_mean[0]\n",
    "        denormed_errors = denormed_expected - denormed_prediction\n",
    "\n",
    "        # standardized MAE\n",
    "        absolute_errors = tf.abs(batch_labels - prediction)\n",
    "        pre_MAE = tf.math.reduce_sum(absolute_errors)/horizon_batches\n",
    "        MAE += pre_MAE.numpy().flatten().tolist()\n",
    "\n",
    "        # MAE\n",
    "        denormed_absolute_errors = tf.abs(denormed_errors)\n",
    "        denormed_pre_MAE = tf.math.reduce_sum(denormed_absolute_errors)/horizon_batches\n",
    "        denormed_MAE += denormed_pre_MAE.numpy().flatten().tolist()\n",
    "\n",
    "        # MAPE: https://otexts.com/fpp2/accuracy.html\n",
    "        denormed_pre_MAPE = tf.math.reduce_sum(tf.abs(100 * (denormed_errors/denormed_expected)))/horizon_batches\n",
    "        denormed_MAPE += denormed_pre_MAPE.numpy().flatten().tolist()\n",
    "\n",
    "        # sMAPE: M4 contest paper\n",
    "        denormed_pre_sMAPE = (2 * tf.math.reduce_sum(denormed_absolute_errors/(tf.abs(denormed_expected) + tf.abs(denormed_prediction))))/horizon_batches\n",
    "        denormed_sMAPE += denormed_pre_sMAPE.numpy().flatten().tolist()\n",
    "\n",
    "        # MASE: M4 contest paper\n",
    "        denormed_naive_m = (batch_inputs[:, :, 0:1] * train_std[0]) + train_mean[0]\n",
    "        denormed_pre_MASE = (denormed_absolute_errors/(tf.abs(denormed_expected - denormed_naive_m)/m_interval)) / horizon_batches\n",
    "\n",
    "        denormed_MASE += denormed_pre_MASE.numpy().flatten().tolist()\n",
    "\n",
    "    return np.array(MAE).mean(), np.array(denormed_MAE).mean(), 100.0 * np.array(denormed_sMAPE).mean(), np.array(denormed_MAPE).mean(), np.array(denormed_MASE).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def delete_model(model):\n",
    "    \"\"\"\n",
    "    Memory Handling: Clear a tensorflow model from memory & with garbage collector.\n",
    "    :param model: Tensorflow model to remove.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Memory handling\n",
    "    del model  # Manually delete model\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reload_default_model_weights(model, model_name):\n",
    "    model.load_weights('{}.h5'.format(model_name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def store_model_weights(model, model_name):\n",
    "    model.save_weights('{}.h5'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reset_model_weights(model):\n",
    "    for ix, layer in enumerate(model.layers):\n",
    "        if hasattr(model.layers[ix], 'kernel_initializer') and hasattr(model.layers[ix], 'bias_initializer'):\n",
    "            weight_initializer = model.layers[ix].kernel_initializer\n",
    "            bias_initializer = model.layers[ix].bias_initializer\n",
    "\n",
    "            old_weights, old_biases = model.layers[ix].get_weights()\n",
    "\n",
    "            model.layers[ix].set_weights([\n",
    "            weight_initializer(shape=old_weights.shape),\n",
    "            bias_initializer(shape=old_biases.shape)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compile_and_fit(model, window, patience=stop_early_patience, verbosity=1):\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=patience, mode='min', restore_best_weights=True)\n",
    "\n",
    "    # model = reset_model_weights(model)\n",
    "\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "    # model.compile(loss=tf.losses.MeanAbsoluteError(),\n",
    "                  optimizer=tf.optimizers.Adam(),\n",
    "                  # optimizer=tf.optimizers.Nadam(),\n",
    "                  metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    history = model.fit(window.train, epochs=MAX_EPOCHS, validation_data=window.val,\n",
    "                        callbacks=[early_stopping], verbose=verbosity)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compile_store_and_fit(model, window, model_name, patience=stop_early_patience):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "    patience=patience,\n",
    "    mode='min', restore_best_weights=True)\n",
    "\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "    # model.compile(loss=tf.losses.MeanAbsoluteError(),\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    # optimizer=tf.optimizers.Nadam(),\n",
    "    metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "\n",
    "    # TODO: store weights first on 1 epoch\n",
    "    model.fit(window.train, epochs=1,\n",
    "    validation_data=window.val,\n",
    "    callbacks=[early_stopping])\n",
    "    print(model.summary())\n",
    "    store_model_weights(model, model_name)\n",
    "\n",
    "    history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "    validation_data=window.val,\n",
    "    callbacks=[early_stopping])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Train the model and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = compile_and_fit(linear, single_step_window)\n",
    "\n",
    "val_performance['Linear'] = linear.evaluate(single_step_window.val)\n",
    "performance['Linear'] = linear.evaluate(single_step_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Like the `baseline` model, the linear model can be called on batches of wide windows. Used this way the model makes a set of independent predictions on consecutive time steps. The `time` axis acts like another `batch` axis. There are no interactions between the predictions at each time step.\n",
    "\n",
    "![A single step prediction](images/wide_window.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Input shape:', wide_window.example[0].shape)\n",
    "print('Output shape:', baseline(wide_window.example[0]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here is the plot of its example predictions on the `wide_window`, note how in many cases the prediction is clearly better than just returning the input temperature, but in a few cases it's worse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wide_window.plot(linear, model_title=\"Linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "One advantage to linear models is that they're relatively simple to  interpret.\n",
    "You can pull out the layer's weights, and see the weight assigned to each input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plt.bar(x = range(len(train_df.columns)),\n",
    "#         height=linear.layers[0].kernel[:,0].numpy())\n",
    "# axis = plt.gca()\n",
    "# axis.set_xticks(range(len(train_df.columns)))\n",
    "# _ = axis.set_xticklabels(train_df.columns, rotation=90)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [5, 8]\n",
    "ax = plt.barh(width=linear.layers[0].kernel[:,0].numpy(), y = range(len(train_df.columns)))\n",
    "axis = plt.gca()\n",
    "axis.set_yticks(range(len(train_df.columns)))\n",
    "_ = axis.set_yticklabels(train_df.columns)\n",
    "\n",
    "l = []\n",
    "for i in range(len(train_df.columns)):\n",
    "    l.append([train_df.columns[i], round(linear.layers[0].kernel[:,0].numpy()[i], 2)])\n",
    "l.sort(key= lambda x: x[1])\n",
    "l.reverse()\n",
    "for l1 in l:\n",
    "    print(l1)\n",
    "plt.title(\"DR linear: Signal weights of 1st layer ($\\\\approx \\\\in [-1, 1]$)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "linear.layers[0].kernel[:,0].numpy()[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Sometimes the model doesn't even place the most weight on the input `T (degC)`. This is one of the risks of random initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Multi-step models\n",
    "\n",
    "Both the single-output and multiple-output models in the previous sections made **single time step predictions**, 1h into the future.\n",
    "\n",
    "This section looks at how to expand these models to make **multiple time step predictions**.\n",
    "\n",
    "In a multi-step prediction, the model needs to learn to predict a range of future values. Thus, unlike a single step model, where only a single future point is predicted, a multi-step model predicts a sequence of the future values.\n",
    "\n",
    "There are two rough approaches to this:\n",
    "\n",
    "1. Single shot predictions where the entire time series is predicted at once.\n",
    "2. Autoregressive predictions where the model only makes single step predictions and its output is fed back as its input.\n",
    "\n",
    "In this section all the models will predict **all the features across all output time steps**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For the multi-step model, the training data again consists of hourly samples. However, here, the models will learn to predict 24h of the future, given 24h of the past.\n",
    "\n",
    "Here is a `Window` object that generates these slices from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seed += 1\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "OUT_STEPS = 24\n",
    "mode = \"MISO\"  # MIMO\n",
    "INPUT_WIDTH = 24\n",
    "\n",
    "if mode == \"MIMO\":\n",
    "    print(\"MIMO window\")\n",
    "    multi_window = WindowGenerator(input_width=INPUT_WIDTH,\n",
    "                                   label_width=OUT_STEPS,\n",
    "                                   shift=OUT_STEPS)\n",
    "else:\n",
    "    print(\"MISO window\")\n",
    "    multi_window = WindowGenerator(input_width=INPUT_WIDTH,\n",
    "                                   label_width=OUT_STEPS,\n",
    "                                   shift=OUT_STEPS,\n",
    "                                   label_columns=['Load (kW)'])\n",
    "    num_features = 1\n",
    "\n",
    "multi_window.plot(model_title=\"48h rolling windows -\")\n",
    "multi_window\n",
    "\n",
    "for example_inputs, example_labels in multi_window.train.take(1):\n",
    "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "  print(f'Labels shape (batch, time, features): {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A simple baseline for this task is to repeat the last input time step for the required number of output timesteps:\n",
    "\n",
    "![Repeat the last input, for each output step](images/multistep_last.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiStepLastBaseline(tf.keras.Model):\n",
    "  def call(self, inputs):\n",
    "    return tf.tile(inputs[:, -1:, :], [1, OUT_STEPS, 1])\n",
    "\n",
    "last_baseline = MultiStepLastBaseline()\n",
    "last_baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "# last_baseline.compile(loss=tf.losses.MeanAbsoluteError(),\n",
    "                      metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "multi_val_performance = {}\n",
    "multi_performance = {}\n",
    "\n",
    "multi_val_performance['Last'] = last_baseline.evaluate(multi_window.val)\n",
    "multi_performance['Last'] = last_baseline.evaluate(multi_window.test, verbose=0)\n",
    "standardized_MAE, MAE, sMAPE, MAPE, MASE = \\\n",
    "    evaluate_denormed_all(multi_window.test, multi_window.test_df, last_baseline)\n",
    "print(\"MAE: {}, Denormed MAE: {} sMAPE%: {}%, MAPE%: {}%, MASE: {}\"\n",
    "      .format(round(standardized_MAE, 2), round(MAE, 2), round(sMAPE, 2), round(MAPE, 2), round(MASE, 2)))\n",
    "multi_window.plot(last_baseline, model_title=\"Last 1h -\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since this task is to predict 24h given 24h another simple approach is to repeat the previous day, assuming tomorrow will be similar:\n",
    "\n",
    "![Repeat the previous day](images/multistep_repeat.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RepeatBaseline(tf.keras.Model):\n",
    "  def call(self, inputs):\n",
    "    return inputs[:, -OUT_STEPS:, :]\n",
    "\n",
    "repeat_baseline = RepeatBaseline()\n",
    "repeat_baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "# repeat_baseline.compile(loss=tf.losses.MeanAbsoluteError(),\n",
    "                        metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "multi_val_performance['Repeat'] = repeat_baseline.evaluate(multi_window.val)\n",
    "multi_performance['Repeat'] = repeat_baseline.evaluate(multi_window.test, verbose=0)\n",
    "standardized_MAE, MAE, sMAPE, MAPE, MASE = \\\n",
    "    evaluate_denormed_all(multi_window.test, multi_window.test_df, repeat_baseline)\n",
    "print(\"MAE: {}, Denormed MAE: {} sMAPE%: {}%, MAPE%: {}%, MASE: {}\"\n",
    "      .format(round(standardized_MAE, 2), round(MAE, 2), round(sMAPE, 2), round(MAPE, 2), round(MASE, 2)))\n",
    "multi_window.plot(repeat_baseline, model_title=\"Repeat 24h -\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Repeat 24 hours from 7 days ago:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Repeat6d24hBaseline(tf.keras.Model):\n",
    "  def call(self, inputs):\n",
    "    feature_count = inputs.shape[2]\n",
    "    return inputs[:, :, feature_count-1:feature_count]\n",
    "\n",
    "repeat_6d24h_baseline = Repeat6d24hBaseline()\n",
    "repeat_6d24h_baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "# repeat_6d24h_baseline.compile(loss=tf.losses.MeanAbsoluteError(),\n",
    "                        metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "multi_val_performance['Repeat6d24h'] = repeat_6d24h_baseline.evaluate(multi_window.val)\n",
    "multi_performance['Repeat6d24h'] = repeat_6d24h_baseline.evaluate(multi_window.test, verbose=0)\n",
    "standardized_MAE, MAE, sMAPE, MAPE, MASE = \\\n",
    "    evaluate_denormed_all(multi_window.test, multi_window.test_df, repeat_6d24h_baseline)\n",
    "print(\"MAE: {}, Denormed MAE: {} sMAPE%: {}%, MAPE%: {}%, MASE: {}\"\n",
    "      .format(round(standardized_MAE, 2), round(MAE, 2), round(sMAPE, 2), round(MAPE, 2), round(MASE, 2)))\n",
    "multi_window.plot(repeat_6d24h_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Repeat7d24hBaseline(tf.keras.Model):\n",
    "  def call(self, inputs):\n",
    "    feature_count = inputs.shape[2]\n",
    "    return inputs[:, :, feature_count-2:feature_count-1]\n",
    "\n",
    "repeat_7d24h_baseline = Repeat7d24hBaseline()\n",
    "repeat_7d24h_baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "# repeat_7d24h_baseline.compile(loss=tf.losses.MeanAbsoluteError(),\n",
    "                        metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "multi_val_performance['Repeat7d24h'] = repeat_7d24h_baseline.evaluate(multi_window.val)\n",
    "multi_performance['Repeat7d24h'] = repeat_7d24h_baseline.evaluate(multi_window.test, verbose=0)\n",
    "standardized_MAE, MAE, sMAPE, MAPE, MASE = \\\n",
    "    evaluate_denormed_all(multi_window.test, multi_window.test_df, repeat_7d24h_baseline)\n",
    "print(\"MAE: {}, Denormed MAE: {} sMAPE%: {}%, MAPE%: {}%, MASE: {}\"\n",
    "      .format(round(standardized_MAE, 2), round(MAE, 2), round(sMAPE, 2), round(MAPE, 2), round(MASE, 2)))\n",
    "multi_window.plot(repeat_7d24h_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Repeat8d24hBaseline(tf.keras.Model):\n",
    "  def call(self, inputs):\n",
    "    feature_count = inputs.shape[2]\n",
    "    return inputs[:, :, feature_count-3:feature_count-2]\n",
    "\n",
    "repeat_8d24h_baseline = Repeat8d24hBaseline()\n",
    "repeat_8d24h_baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "# repeat_8d24h_baseline.compile(loss=tf.losses.MeanAbsoluteError(),\n",
    "                        metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "multi_val_performance['Repeat8d24h'] = repeat_8d24h_baseline.evaluate(multi_window.val)\n",
    "multi_performance['Repeat8d24h'] = repeat_8d24h_baseline.evaluate(multi_window.test, verbose=0)\n",
    "standardized_MAE, MAE, sMAPE, MAPE, MASE = \\\n",
    "    evaluate_denormed_all(multi_window.test, multi_window.test_df, repeat_8d24h_baseline)\n",
    "print(\"MAE: {}, Denormed MAE: {} sMAPE%: {}%, MAPE%: {}%, MASE: {}\"\n",
    "      .format(round(standardized_MAE, 2), round(MAE, 2), round(sMAPE, 2), round(MAPE, 2), round(MASE, 2)))\n",
    "multi_window.plot(repeat_8d24h_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Single-shot models\n",
    "\n",
    "One high level approach to this problem is use a \"single-shot\" model, where the model makes the entire sequence prediction in a single step.\n",
    "\n",
    "This can be implemented efficiently as a `layers.Dense` with `OUT_STEPS*features` output units. The model just needs to reshape that output to the required `(OUTPUT_STEPS, features)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Linear\n",
    "\n",
    "A simple linear model based on the last input time step does better than either baseline, but is underpowered. The model needs to predict `OUTPUT_STEPS` time steps, from a single input time step with a linear projection. It can only capture a low-dimensional slice of the behavior, likely based mainly on the time of day and time of year.\n",
    "\n",
    "![Predict all timesteps from the last time-step](images/multistep_dense.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "multi_linear_model = tf.keras.Sequential([\n",
    "    # Take the last time-step.\n",
    "    # Shape [batch, time, features] => [batch, 1, features]\n",
    "    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "    # Shape => [batch, 1, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n",
    "# history = compile_and_fit(multi_linear_model, multi_window)\n",
    "history = compile_store_and_fit(multi_linear_model, multi_window, \"Linear\")\n",
    "\n",
    "IPython.display.clear_output()\n",
    "multi_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val)\n",
    "multi_performance['Linear'] = multi_linear_model.evaluate(multi_window.test, verbose=0)\n",
    "standardized_MAE, MAE, sMAPE, MAPE, MASE = \\\n",
    "    evaluate_denormed_all(multi_window.test, multi_window.test_df, multi_linear_model)\n",
    "print(\"MAE: {}, Denormed MAE: {} sMAPE%: {}%, MAPE%: {}%, MASE: {}\"\n",
    "      .format(round(standardized_MAE, 2), round(MAE, 2), round(sMAPE, 2), round(MAPE, 2), round(MASE, 2)))\n",
    "multi_window.plot(multi_linear_model, model_title=\"Linear -\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_window.test\n",
    "print(multi_linear_model.predict(multi_window.test).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Dense\n",
    "\n",
    "Adding a `layers.Dense` between the input and output gives the linear model more power, but is still only based on a single input timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "multi_dense_model = tf.keras.Sequential([\n",
    "    # Take the last time step.\n",
    "    # Shape [batch, time, features] => [batch, 1, features]\n",
    "    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "    # Shape => [batch, 1, dense_units]\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n",
    "# history = compile_and_fit(multi_dense_model, multi_window)\n",
    "history = compile_store_and_fit(multi_dense_model, multi_window, \"Dense\")\n",
    "IPython.display.clear_output()\n",
    "multi_val_performance['Dense'] = multi_dense_model.evaluate(multi_window.val)\n",
    "multi_performance['Dense'] = multi_dense_model.evaluate(multi_window.test, verbose=0)\n",
    "standardized_MAE, MAE, sMAPE, MAPE, MASE = \\\n",
    "    evaluate_denormed_all(multi_window.test, multi_window.test_df, multi_dense_model)\n",
    "print(\"MAE: {}, Denormed MAE: {} sMAPE%: {}%, MAPE%: {}%, MASE: {}\"\n",
    "      .format(round(standardized_MAE, 2), round(MAE, 2), round(sMAPE, 10), round(MAPE, 2), round(MASE, 2)))\n",
    "multi_window.plot(multi_dense_model, model_title=\"Dense -\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "std = train_std[0]\n",
    "mean = train_mean[0]\n",
    "def evaluate_denormed_MASE2(data, df, model, batch_size=32, forecast_horizon=24, m_interval=24):\n",
    "    denormed_MASE = []\n",
    "    x = df.shape[0]\n",
    "    for batch_inputs, batch_labels in data.take(int(x/batch_size)):\n",
    "        prediction = model.predict(batch_inputs)\n",
    "\n",
    "        horizon_batches = batch_inputs.shape[0] * forecast_horizon\n",
    "\n",
    "        denormed_prediction = (prediction * std) + mean\n",
    "        denormed_expected = (batch_labels * std) + mean\n",
    "        denormed_naive_m = (batch_inputs[:, :, 0:1] * std) + mean\n",
    "\n",
    "        denormed_pre_MASE = (tf.abs(denormed_expected - denormed_prediction)\n",
    "                             /(tf.abs(denormed_expected - denormed_naive_m)/m_interval))\\\n",
    "                            / horizon_batches\n",
    "        denormed_MASE += denormed_pre_MASE.numpy().tolist()\n",
    "\n",
    "    return np.array(denormed_MASE).mean()\n",
    "MASE = evaluate_denormed_MASE2(multi_window.test, multi_window.test_df, multi_dense_model)\n",
    "print(\"MASE: {}\".format(round(MASE, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def denormed_mae(y_true, y_pred):\n",
    "    denormed_mae = tf.metrics.MAE((y_true * train_std[0] + train_mean[0]), (y_pred * train_std[0] + train_mean[0]))\n",
    "    return tf.reduce_mean(denormed_mae, axis=-1)\n",
    "\n",
    "def denormed_mape(y_true, y_pred):\n",
    "    denormed_mape = tf.metrics.MAPE((y_true * train_std[0] + train_mean[0]), (y_pred * train_std[0] + train_mean[0]))\n",
    "    return tf.reduce_mean(denormed_mape, axis=-1)\n",
    "\n",
    "def denormed_smape(y_true, y_pred):\n",
    "    denormed_prediction = (y_pred * train_std[0]) + train_mean[0]\n",
    "    denormed_expected = (y_true * train_std[0]) + train_mean[0]\n",
    "\n",
    "    denormed_residuals = tf.abs(denormed_expected - denormed_prediction)\n",
    "    denormed_denominator = tf.abs(denormed_expected) + tf.abs(denormed_prediction)\n",
    "    denormed_pre_sMAPE = 200.0 * (denormed_residuals/denormed_denominator)\n",
    "    return tf.reduce_mean(denormed_pre_sMAPE, axis=-1)\n",
    "\n",
    "multi_dense_model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "    # model.compile(loss=tf.losses.MeanAbsoluteError(),\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    # optimizer=tf.optimizers.Nadam(),\n",
    "    metrics=[denormed_mae,\n",
    "             denormed_mape,\n",
    "             denormed_smape\n",
    "             ])\n",
    "\n",
    "standardized_MAE, MAE, sMAPE, MAPE, MASE = \\\n",
    "    evaluate_denormed_all(multi_window.test, multi_window.test_df, multi_dense_model)\n",
    "print(\"MAE: {}, Denormed MAE: {} sMAPE%: {}%, MAPE%: {}%, MASE: {}\"\n",
    "      .format(round(standardized_MAE, 8), round(MAE, 8), round(sMAPE, 8), round(MAPE, 8), round(MASE, 8)))\n",
    "\n",
    "z = multi_dense_model.evaluate(multi_window.test)\n",
    "l = multi_dense_model.metrics_names\n",
    "for i in range(len(z)):\n",
    "    print(round(z[i], 8), l[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A convolutional model makes predictions based on a fixed-width history, which may lead to better performance than the dense model since it can see how things are changing over time:\n",
    "\n",
    "![A convolutional model sees how things change over time](images/multistep_conv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "CONV_WIDTH = 3\n",
    "multi_conv_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n",
    "    tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n",
    "    # Shape => [batch, 1, conv_units]\n",
    "    tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)),\n",
    "    # Shape => [batch, 1,  out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n",
    "history = compile_store_and_fit(multi_conv_model, multi_window, \"CNN\")\n",
    "IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance['Conv'] = multi_conv_model.evaluate(multi_window.val)\n",
    "multi_performance['Conv'] = multi_conv_model.evaluate(multi_window.test, verbose=0)\n",
    "standardized_MAE, MAE, sMAPE, MAPE, MASE = \\\n",
    "    evaluate_denormed_all(multi_window.test, multi_window.test_df, multi_conv_model)\n",
    "print(\"MAE: {}, Denormed MAE: {} sMAPE%: {}%, MAPE%: {}%, MASE: {}\"\n",
    "      .format(round(standardized_MAE, 2), round(MAE, 2), round(sMAPE, 2), round(MAPE, 2), round(MASE, 2)))\n",
    "multi_window.plot(multi_conv_model, model_title=\"CNN -\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A recurrent model can learn to use a long history of inputs, if it's relevant to the predictions the model is making. Here the model will accumulate internal state for 24h, before making a single prediction for the next 24h.\n",
    "\n",
    "In this single-shot format, the LSTM only needs to produce an output at the last time step, so set `return_sequences=False`.\n",
    "\n",
    "![The lstm accumulates state over the input window, and makes a single prediction for the next 24h](images/multistep_lstm.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "multi_lstm_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\n",
    "    # Adding more `lstm_units` just overfits more quickly.\n",
    "    # tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n",
    "history = compile_store_and_fit(multi_lstm_model, multi_window, \"LSTM\")\n",
    "\n",
    "IPython.display.clear_output()\n",
    "multi_val_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.val)\n",
    "multi_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.test, verbose=0)\n",
    "standardized_MAE, MAE, sMAPE, MAPE, MASE = \\\n",
    "    evaluate_denormed_all(multi_window.test, multi_window.test_df, multi_lstm_model)\n",
    "print(\"MAE: {}, Denormed MAE: {} sMAPE%: {}%, MAPE%: {}%, MASE: {}\"\n",
    "      .format(round(standardized_MAE, 2), round(MAE, 2), round(sMAPE, 2), round(MAPE, 2), round(MASE, 2)))\n",
    "multi_window.plot(multi_conv_model, model_title=\"LSTM -\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_window.test_df.head(25)[\"Load (kW)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_window.test_df['Load (kW)'].index[(np.abs(multi_window.test_df['Load (kW)'] - 0.39829713) < 0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_window.test_df['Load (kW)'].index[(np.abs(multi_window.test_df['Load (kW)'] - 0.398297) < 0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_window.test_df['Load (kW)'].index[(np.abs(multi_window.test_df['Load (kW)'] - 0.3122604) < 0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_window.test_df['Load (kW)'].index[(np.abs(multi_window.test_df['Load (kW)'] - -1.254899) < 0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_window.test_df['Load (kW)'].index[(np.abs(multi_window.test_df['Load (kW)'] - -0.95466876) < 0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_window.test_df['Load (kW)'].index[(np.abs(multi_window.test_df['Load (kW)'] - -0.8932725) < 0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig_multi_window_lstm = multi_window.plot(multi_lstm_model, model_title=\"LSTM -\")\n",
    "# fig_multi_window_lstm.save\n",
    "plt.grid(True)\n",
    "plt.savefig('fig_multi_window_lstm.pdf', bbox_inches=\"tight\", dpi=600)\n",
    "plt.savefig('fig_multi_window_lstm.svg', bbox_inches=\"tight\", dpi=600)\n",
    "plt.savefig('fig_multi_window_lstm.png', bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "multi_gru_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\n",
    "    # Adding more `gru_units` just overfits more quickly.\n",
    "    tf.keras.layers.GRU(64, return_sequences=False),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n",
    "history = compile_store_and_fit(multi_gru_model, multi_window, \"GRU\")\n",
    "\n",
    "IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance['GRU'] = multi_gru_model.evaluate(multi_window.val)\n",
    "multi_performance['GRU'] = multi_gru_model.evaluate(multi_window.test, verbose=2)\n",
    "standardized_MAE, MAE, sMAPE, MAPE, MASE = \\\n",
    "    evaluate_denormed_all(multi_window.test, multi_window.test_df, multi_gru_model)\n",
    "print(\"MAE: {}, Denormed MAE: {} sMAPE%: {}%, MAPE%: {}%, MASE: {}\"\n",
    "      .format(round(standardized_MAE, 2), round(MAE, 2), round(sMAPE, 2), round(MAPE, 2), round(MASE, 2)))\n",
    "multi_window.plot(multi_gru_model, model_title=\"GRU -\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### TODO: Quasi SVM (see: https://keras.io/examples/keras_recipes/quasi_svm/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.layers.experimental import RandomFourierFeatures\n",
    "#\n",
    "# multi_quasi_svm_model = tf.keras.Sequential([\n",
    "#     # Shape [batch, time, features] => [batch, lstm_units]\n",
    "#     # Adding more `gru_units` just overfits more quickly.\n",
    "#     # tf.keras.layers.GRU(32, return_sequences=False),\n",
    "#     # tf.keras.layers.GRU(64, return_sequences=False),\n",
    "#     RandomFourierFeatures(\n",
    "#             output_dim=4096, scale=10.0, kernel_initializer=\"gaussian\"\n",
    "#         ),\n",
    "#     # Shape => [batch, out_steps*features]\n",
    "#     tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "#                           kernel_initializer=tf.initializers.zeros),\n",
    "#     # Shape => [batch, out_steps, features]\n",
    "#     tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "# ])\n",
    "#\n",
    "# history = compile_store_and_fit(multi_quasi_svm_model, multi_window, \"GRU\")\n",
    "#\n",
    "# IPython.display.clear_output()\n",
    "#\n",
    "# multi_val_performance['QuasiSVM'] = multi_quasi_svm_model.evaluate(multi_window.val)\n",
    "# multi_performance['QuasiSVM'] = multi_quasi_svm_model.evaluate(multi_window.test, verbose=2)\n",
    "# multi_window.plot(multi_quasi_svm_model, model_title=\"QuasiSVM -\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## TCN (see: https://github.com/philipperemy/keras-tcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tcn import TCN, tcn_full_summary\n",
    "\n",
    "# The receptive field tells you how far the model can see in terms of timesteps.\n",
    "tcn_layer = TCN(64, return_sequences=False)  # hour x 24 x 7\n",
    "tcn_layer_receptive_field = tcn_layer.receptive_field\n",
    "\n",
    "print(\"Receptive field size = \\n\\t{} hours, {} days, {} weeks, {} months\"\n",
    "      .format(tcn_layer_receptive_field,\n",
    "              round(tcn_layer_receptive_field/24.0, 2),\n",
    "              round(tcn_layer_receptive_field/(24.0 * 7.0), 2),\n",
    "              round(tcn_layer_receptive_field/(24.0 * 30.0), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "multi_tcn_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\n",
    "    TCN(64, return_sequences=False),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n",
    "history = compile_store_and_fit(multi_tcn_model, multi_window, \"TCN\")\n",
    "IPython.display.clear_output()\n",
    "multi_val_performance['TCN'] = multi_tcn_model.evaluate(multi_window.val)\n",
    "multi_performance['TCN'] = multi_tcn_model.evaluate(multi_window.test, verbose=0)\n",
    "standardized_MAE, MAE, sMAPE, MAPE, MASE = \\\n",
    "    evaluate_denormed_all(multi_window.test, multi_window.test_df, multi_tcn_model)\n",
    "print(\"MAE: {}, Denormed MAE: {} sMAPE%: {}%, MAPE%: {}%, MASE: {}\"\n",
    "      .format(round(standardized_MAE, 2), round(MAE, 2), round(sMAPE, 2), round(MAPE, 2), round(MASE, 2)))\n",
    "multi_window.plot(multi_tcn_model, model_title=\"TCN -\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### TCN kernel_size=12, dropout_rate=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "multi_tcn_dropout_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\n",
    "    TCN(64, return_sequences=False, kernel_size=24, dropout_rate=0.25),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n",
    "history = compile_store_and_fit(multi_tcn_dropout_model, multi_window, \"TCN\")\n",
    "IPython.display.clear_output()\n",
    "multi_val_performance['TCN dropout'] = multi_tcn_dropout_model.evaluate(multi_window.val)\n",
    "multi_performance['TCN dropout'] = multi_tcn_dropout_model.evaluate(multi_window.test, verbose=0)\n",
    "standardized_MAE, MAE, sMAPE, MAPE, MASE = \\\n",
    "    evaluate_denormed_all(multi_window.test, multi_window.test_df, multi_tcn_dropout_model)\n",
    "print(\"MAE: {}, Denormed MAE: {} sMAPE%: {}%, MAPE%: {}%, MASE: {}\"\n",
    "      .format(round(standardized_MAE, 2), round(MAE, 2), round(sMAPE, 2), round(MAPE, 2), round(MASE, 2)))\n",
    "multi_window.plot(multi_tcn_dropout_model, model_title=\"TCN dropout -\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## LSTM Attention (see: https://github.com/philipperemy/keras-attention-mechanism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from attention import Attention\n",
    "multi_lstm_attention_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    Attention(32),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n",
    "history = compile_store_and_fit(multi_lstm_attention_model, multi_window, \"LSTM Attention\")\n",
    "\n",
    "IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance['LSTM Attention'] = multi_lstm_attention_model.evaluate(multi_window.val)\n",
    "multi_performance['LSTM Attention'] = multi_lstm_attention_model.evaluate(multi_window.test, verbose=0)\n",
    "standardized_MAE, MAE, sMAPE, MAPE, MASE = \\\n",
    "    evaluate_denormed_all(multi_window.test, multi_window.test_df, multi_lstm_attention_model)\n",
    "print(\"MAE: {}, Denormed MAE: {} sMAPE%: {}%, MAPE%: {}%, MASE: {}\"\n",
    "      .format(round(standardized_MAE, 2), round(MAE, 2), round(sMAPE, 2), round(MAPE, 2), round(MASE, 2)))\n",
    "multi_window.plot(multi_lstm_attention_model, model_title=\"LSTM Attention -\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## GRU TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "multi_gru_tcn_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\n",
    "    # Adding more `gru_units` just overfits more quickly.\n",
    "    TCN(32, return_sequences=True),\n",
    "    tf.keras.layers.GRU(32, return_sequences=False),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n",
    "history = compile_store_and_fit(multi_gru_tcn_model, multi_window, \"GRU TCN\")\n",
    "\n",
    "IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance['GRU TCN'] = multi_gru_tcn_model.evaluate(multi_window.val)\n",
    "multi_performance['GRU TCN'] = multi_gru_tcn_model.evaluate(multi_window.test, verbose=2)\n",
    "standardized_MAE, MAE, sMAPE, MAPE, MASE = \\\n",
    "    evaluate_denormed_all(multi_window.test, multi_window.test_df, multi_gru_tcn_model)\n",
    "print(\"MAE: {}, Denormed MAE: {} sMAPE%: {}%, MAPE%: {}%, MASE: {}\"\n",
    "      .format(round(standardized_MAE, 2), round(MAE, 2), round(sMAPE, 2), round(MAPE, 2), round(MASE, 2)))\n",
    "multi_window.plot(multi_gru_tcn_model, model_title=\"GRU TCN -\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_lstm_attention_model.evaluate(multi_window.test, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## TODO: BNN (see: https://keras.io/examples/keras_recipes/bayesian_neural_networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## TFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow_probability as tfp\n",
    "# tfd = tfp.distributions\n",
    "#\n",
    "# NUM_TRAIN_EXAMPLES = train_df.shape[0]\n",
    "# kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /  # pylint: disable=g-long-lambda\n",
    "#                         tf.cast(NUM_TRAIN_EXAMPLES, dtype=tf.float32))\n",
    "#\n",
    "# multi_flipout_model = tf.keras.Sequential([\n",
    "#     # Take the last time-step.\n",
    "#     # Shape [batch, time, features] => [batch, 1, features]\n",
    "#     tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "#     # Shape => [batch, 1, out_steps*features]\n",
    "#     # tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros),\n",
    "#     # tfp.layers.DenseFlipout(OUT_STEPS*num_features),\n",
    "#     # tf.keras.layers.BatchNormalization(),\n",
    "#     tfp.layers.DenseFlipout(OUT_STEPS*num_features,\n",
    "#             kernel_divergence_fn=kl_divergence_function,\n",
    "#           activation=tf.nn.relu),\n",
    "#   # tfp.layers.DistributionLambda(\n",
    "#   #     lambda t: tfd.Normal(loc=t[..., :1],\n",
    "#   #                          scale=1e-3 + tf.math.softplus(0.05 * t[...,1:]))),\n",
    "#     # Shape => [batch, out_steps, features]\n",
    "#     tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "# ])\n",
    "#\n",
    "# # multi_gru_model = tf.keras.Sequential([\n",
    "# #     # Shape [batch, time, features] => [batch, lstm_units]\n",
    "# #     # Adding more `gru_units` just overfits more quickly.\n",
    "# #     tf.keras.layers.GRU(32, return_sequences=False),\n",
    "# #     # Shape => [batch, out_steps*features]\n",
    "# #     tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "# #                           kernel_initializer=tf.initializers.zeros),\n",
    "# #     # Shape => [batch, out_steps, features]\n",
    "# #     tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "# # ])\n",
    "#\n",
    "# history = compile_and_fit(multi_flipout_model, multi_window)\n",
    "#\n",
    "# IPython.display.clear_output()\n",
    "#\n",
    "# multi_val_performance['Flipout'] = multi_flipout_model.evaluate(multi_window.val)\n",
    "# multi_performance['Flipout'] = multi_flipout_model.evaluate(multi_window.test, verbose=2)\n",
    "# multi_window.plot(multi_flipout_model, model_title=\"Flipout -\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are clearly diminishing returns as a function of model complexity on this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(len(multi_performance))\n",
    "width = 0.3\n",
    "\n",
    "metric_name = 'mean_absolute_error'\n",
    "metric_index = multi_lstm_model.metrics_names.index('mean_absolute_error')\n",
    "val_mae = [v[metric_index] for v in multi_val_performance.values()]\n",
    "test_mae = [v[metric_index] for v in multi_performance.values()]\n",
    "\n",
    "plt.bar(x - 0.17, val_mae, width, label='Validation')\n",
    "plt.bar(x + 0.17, test_mae, width, label='Test')\n",
    "plt.xticks(ticks=x, labels=multi_performance.keys(),\n",
    "           rotation=45)\n",
    "plt.ylabel(f'MAE (average over all times and outputs)')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The metrics for the multi-output models in the first half of this tutorial show the performance averaged across all output features. These performances similar but also averaged across output timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for name, value in multi_performance.items():\n",
    "  print(f'{name:8s}: {value[1]:0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Manual Baseline MAE\\n\")\n",
    "\n",
    "normed_mae_4d = np.mean(np.abs(test_df[\"Load (kW)\"] - test_df['Load (kW)'].shift(24*4)))\n",
    "normed_mae_5d = np.mean(np.abs(test_df[\"Load (kW)\"] - test_df['Load (kW)'].shift(24*5)))\n",
    "normed_mae_6d = np.mean(np.abs(test_df[\"Load (kW)\"] - test_df['Load (kW)'].shift(24*6)))\n",
    "normed_mae_7d = np.mean(np.abs(test_df[\"Load (kW)\"] - test_df['Load (kW)'].shift(24*7)))\n",
    "normed_mae_8d = np.mean(np.abs(test_df[\"Load (kW)\"] - test_df['Load (kW)'].shift(24*8)))\n",
    "normed_mae_9d = np.mean(np.abs(test_df[\"Load (kW)\"] - test_df['Load (kW)'].shift(24*9)))\n",
    "print(f\"Test MAE 4d24h: {normed_mae_4d:0.4f}\")\n",
    "print(f\"Test MAE 5d24h: {normed_mae_5d:0.4f}\")\n",
    "print(f\"Test MAE 6d24h: {normed_mae_6d:0.4f}\")\n",
    "print(f\"Test MAE 7d24h: {normed_mae_7d:0.4f}\")\n",
    "print(f\"Test MAE 8d24h: {normed_mae_8d:0.4f}\")\n",
    "print(f\"Test MAE 9d24h: {normed_mae_9d:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fixed model runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run_count = 100 # 100 # 1\n",
    "model_performances = pd.DataFrame(columns=[\"timestamp\", \"run\", \"model\", \"val_MAE\", \"MAE\", \"denormalized_MAE\",\n",
    "                                           \"denormalized_MASE\", \"denormalized_MAPE\", \"denormalized_sMAPE\", \"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "models = [\n",
    "    (\"Last1h\", last_baseline),\n",
    "    (\"Repeat24h\", repeat_baseline),\n",
    "    (\"Repeat7d24h\", repeat_7d24h_baseline),\n",
    "    (\"Linear\", multi_linear_model),\n",
    "    (\"Dense\", multi_dense_model),\n",
    "    (\"CNN\", multi_conv_model),\n",
    "    (\"LSTM\", multi_lstm_model),\n",
    "    (\"GRU\", multi_gru_model),\n",
    "    (\"TCN\", multi_tcn_model),\n",
    "    (\"LSTM Attention\", multi_lstm_attention_model),\n",
    "]\n",
    "\n",
    "seed = 0\n",
    "for i in range(run_count):\n",
    "    seed += 1\n",
    "    tf.random.set_seed(seed)\n",
    "    for model_name, model_instance in models:\n",
    "        delete_model(model_instance)\n",
    "        if model_name == \"Linear\":\n",
    "            model_instance = tf.keras.Sequential([\n",
    "                tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "                tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros),\n",
    "                tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "            ])\n",
    "        elif model_name == \"Dense\":\n",
    "            model_instance = tf.keras.Sequential([\n",
    "                tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "                tf.keras.layers.Dense(512, activation='relu'),\n",
    "                tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros),\n",
    "                tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "            ])\n",
    "        elif model_name == \"CNN\":\n",
    "            model_instance = tf.keras.Sequential([\n",
    "                tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n",
    "                # tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)),\n",
    "                tf.keras.layers.Conv1D(256, kernel_size=(CONV_WIDTH)),\n",
    "                tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros),\n",
    "                tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "            ])\n",
    "        elif model_name == \"LSTM\":\n",
    "            model_instance = tf.keras.Sequential([\n",
    "                # tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "                tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "                tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros),\n",
    "                tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "            ])\n",
    "        elif model_name == \"GRU\":\n",
    "            model_instance = tf.keras.Sequential([\n",
    "                # tf.keras.layers.GRU(32, return_sequences=False),\n",
    "                tf.keras.layers.GRU(64, return_sequences=False),\n",
    "                tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros),\n",
    "                tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "            ])\n",
    "        elif model_name == \"TCN\":\n",
    "            model_instance = tf.keras.Sequential([\n",
    "                TCN(64, return_sequences=False, kernel_size=24),\n",
    "                tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros),\n",
    "                tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "            ])\n",
    "        elif model_name == \"LSTM Attention\":\n",
    "            model_instance = tf.keras.Sequential([\n",
    "                tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "                Attention(32),\n",
    "                tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros),\n",
    "                tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "            ])\n",
    "\n",
    "        compile_and_fit(model_instance, multi_window, verbosity=0)\n",
    "        standardized_MAE, MAE, sMAPE, MAPE, MASE = evaluate_denormed_all(\n",
    "            multi_window.test, multi_window.test_df, model_instance)\n",
    "        new_row = {\"timestamp\": str(int(time.time())), \"run\": i, \"model\": model_name,\n",
    "            \"val_MAE\": model_instance.evaluate(multi_window.val, verbose=0)[1],\n",
    "            \"MAE\": model_instance.evaluate(multi_window.test, verbose=0)[1],\n",
    "            \"denormalized_MAE\": MAE,\n",
    "            \"denormalized_MASE\": MASE,\n",
    "            \"denormalized_MAPE\": MAPE,\n",
    "            \"denormalized_sMAPE\": sMAPE,\n",
    "            \"type\": model_type}\n",
    "        print(new_row)\n",
    "        model_performances = model_performances.append(new_row, ignore_index=True)\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "datetime_str = now.strftime(\"%m-%d-%Y_%H_%M_%S\")\n",
    "model_performances.to_csv(\"logs\\model_performances_LSTM_GRU_TCN_attention_{}_runs{}_{}.csv\"\n",
    "                          .format(model_type, run_count, datetime_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "sns.set( rc = {'figure.figsize' : ( 20, 20 ), 'axes.labelsize' : 12 })\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "g = sns.catplot(x=\"model\", y=\"MAE\",\n",
    "                hue=\"type\",\n",
    "                col=\"type\",\n",
    "                data=model_performances, kind=\"box\",\n",
    "                # height=4,\n",
    "                # aspect=.7\n",
    "                linewidth=1\n",
    "                )\n",
    "g.fig.suptitle(\"24h-ahead accuracy: DR-DNN vs DNN (train/val: 2017-20, test: 2020-21, runs: {})\".format(run_count) ,\n",
    "               fontsize = 'x-large' ,\n",
    "               fontweight = 'bold' )\n",
    "g.fig.subplots_adjust( top = 0.85 )\n",
    "g.set_axis_labels( \"Model\" , \"Mean Absolute Error (MAE)\", fontsize=16)\n",
    "g.set_xticklabels(rotation=90, fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "sns.set( rc = {'figure.figsize' : ( 20, 20 ), 'axes.labelsize' : 12 })\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "g = sns.catplot(x=\"model\", y=\"denormalized_MAE\",\n",
    "                hue=\"type\",\n",
    "                col=\"type\",\n",
    "                data=model_performances, kind=\"box\",\n",
    "                # height=4,\n",
    "                # aspect=.7\n",
    "                linewidth=1\n",
    "                )\n",
    "g.fig.suptitle(\"24h-ahead accuracy: DR-DNN vs DNN (train/val: 2017-20, test: 2020-21, runs: {})\".format(run_count) ,\n",
    "               fontsize = 'x-large' ,\n",
    "               fontweight = 'bold' )\n",
    "g.fig.subplots_adjust( top = 0.85 )\n",
    "g.set_axis_labels( \"Model\" , \"Load (kW) Mean Absolute Error (MAE)\", fontsize=16)\n",
    "g.set_xticklabels(rotation=90, fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Keras tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## BO LSTM Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compile_and_fit_custom(model, window, patience=stop_early_patience, verbosity=1,\n",
    "                           custom_learning_rate=-1, custom_max_epochs=MAX_EPOCHS):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=patience, mode='min', restore_best_weights=True)\n",
    "    # model = reset_model_weights(model)\n",
    "    if custom_learning_rate != -1:\n",
    "        model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=custom_learning_rate),\n",
    "        metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "    else:\n",
    "        model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      optimizer=tf.optimizers.Adam(),\n",
    "                      metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "    history = model.fit(window.train, epochs=custom_max_epochs, validation_data=window.val,\n",
    "                        callbacks=[early_stopping], verbose=verbosity)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from kerastuner import RandomSearch, Hyperband, BayesianOptimization\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "max_layers = 1\n",
    "min_units = 8 # 8 # 64\n",
    "max_units = 64 # 64 # 512\n",
    "units_step = 8\n",
    "stop_early_patience = 5\n",
    "hyperopt_max_epochs = 350 # 5 # 50 # DNN model iterations\n",
    "max_trials = 20 # 2 # 20 # 50  # BO Hyperopt max iterations\n",
    "hyperopt_runs = 10 # 1 # 10  # Number of times to repeat the hyperparameter search\n",
    "\n",
    "def build_model(hp):\n",
    "\n",
    "    i = 0\n",
    "    # Model: LSTM Attention\n",
    "    current_units = int(round(hp.Int(\"units_\" + str(i), min_value=min_units, max_value=max_units, step=units_step)/2.0, 0))\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(units=current_units, return_sequences=True,\n",
    "                  dropout=hp.Choice(\"dropout\", values=[0.0, 0.01, 0.1, 0.2, 0.25, 0.35, 0.45, 0.5]),\n",
    "                    recurrent_dropout=hp.Choice(\"recurrent_dropout\", values=[0.0, 0.01, 0.1, 0.2, 0.25, 0.35, 0.45, 0.5])\n",
    "                                   ))\n",
    "    model.add(Attention(current_units))\n",
    "    model.add(tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros))\n",
    "    model.add(tf.keras.layers.Reshape([OUT_STEPS, num_features]))\n",
    "\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4]),\n",
    "            # hp.Choice(\"amsgrad\", values=[True, False])\n",
    "        ),\n",
    "        metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "rows = []\n",
    "model_hyperopt_performances = pd.DataFrame(columns=[\"timestamp\", \"run\", \"model\", \"val_MAE\", \"MAE\", \"denormalized_MAE\",\n",
    "                                           \"denormalized_MASE\", \"denormalized_MAPE\", \"denormalized_sMAPE\", \"type\"])\n",
    "for i in range(hyperopt_runs):\n",
    "    # BayesianOptimization  options, see: https://keras.io/api/keras_tuner/tuners/bayesian/\n",
    "    tuner = BayesianOptimization (\n",
    "        build_model,\n",
    "        objective=\"val_loss\",\n",
    "        # factor=3,  # Integer, the reduction factor for the number of epochs and number of models for each bracket. Defaults to 3.\n",
    "        max_trials=max_trials,  # Integer, the total number of trials (model configurations) to test at most. Note that the oracle may interrupt the search before max_trial models have been tested if the search space has been exhausted.\n",
    "        seed=seed,  #  Optional integer, the random seed.\n",
    "        overwrite=True,\n",
    "        num_initial_points=3, # Optional number of randomly generated samples as initial training data for Bayesian optimization. If left unspecified, a value of 3 times the dimensionality of the hyperparameter space is used.\n",
    "        alpha=1e-4, # Float, the value added to the diagonal of the kernel matrix during fitting. It represents the expected amount of noise in the observed performances in Bayesian optimization. Defaults to 1e-4.\n",
    "        beta=2.6, # Float, the balancing factor of exploration and exploitation. The larger it is, the more explorative it is. Defaults to 2.6.\n",
    "        directory=\"keras_tuner_hyperopt\",\n",
    "        project_name=f\"{model_type}_hyperopt\",\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=stop_early_patience,\n",
    "                                                        restore_best_weights=True, mode='min')\n",
    "    # tuner.search(multi_window.train, epochs=MAX_EPOCHS, validation_data=multi_window.val, callbacks=[early_stopping])\n",
    "    tuner.search(multi_window.train, epochs=hyperopt_max_epochs, validation_data=multi_window.val, callbacks=[early_stopping])\n",
    "\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "    tuner.results_summary()\n",
    "\n",
    "    current_units = int(round(best_hp.values[\"units_0\"]/2.0, 0))\n",
    "    best_model = tf.keras.Sequential([\n",
    "                    tf.keras.layers.LSTM(current_units, return_sequences=True),\n",
    "                    Attention(current_units),\n",
    "                    tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros),\n",
    "                    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "                ])\n",
    "\n",
    "    compile_and_fit_custom(best_model, multi_window, verbosity=0,\n",
    "                           custom_learning_rate=best_hp.values[\"learning_rate\"],\n",
    "                           custom_max_epochs=hyperopt_max_epochs)\n",
    "\n",
    "    hyperopt_model_name = f\"LSTM_Attention_extended_hyperopt64_512\"\n",
    "    standardized_MAE, MAE, sMAPE, MAPE, MASE = evaluate_denormed_all(multi_window.test, multi_window.test_df, best_model)\n",
    "    new_row = {\"timestamp\": str(int(time.time())), \"run\": i, \"model\": hyperopt_model_name,\n",
    "        \"val_MAE\": best_model.evaluate(multi_window.val, verbose=0)[1],\n",
    "        \"MAE\": best_model.evaluate(multi_window.test, verbose=0)[1],\n",
    "        \"denormalized_MAE\": MAE,\n",
    "        \"denormalized_MASE\": MASE,\n",
    "        \"denormalized_MAPE\": MAPE,\n",
    "        \"denormalized_sMAPE\": sMAPE,\n",
    "        \"type\": model_type}\n",
    "    print(new_row)\n",
    "    model_hyperopt_performances = model_hyperopt_performances.append(new_row, ignore_index=True)\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "datetime_str = now.strftime(\"%m-%d-%Y_%H_%M_%S\")\n",
    "model_hyperopt_performances.to_csv(\"logs/{}_performances_{}_runs{}_{}.csv\"\n",
    "                      .format(hyperopt_model_name, model_type, hyperopt_runs, datetime_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## BO TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from kerastuner import RandomSearch, Hyperband, BayesianOptimization\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "max_layers = 1\n",
    "min_units = 8 # 8 # 64\n",
    "max_units = 64 # 64 # 512\n",
    "units_step = 8\n",
    "stop_early_patience = 5\n",
    "hyperopt_max_epochs = 150 # 150 # 350 # 5 # 50 # DNN model iterations\n",
    "max_trials = 20 # 2 # 20 # 50  # BO Hyperopt max iterations\n",
    "hyperopt_runs = 10 # 1 # 10  # Number of times to repeat the hyperparameter search\n",
    "min_kernel_size = 1\n",
    "max_kernel_size = 48\n",
    "kernel_size_step = 1\n",
    "\n",
    "def build_model(hp):\n",
    "\n",
    "    i = 0\n",
    "    # Model: TCN\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(TCN(hp.Int(\"units_\" + str(i), min_value=min_units, max_value=max_units, step=units_step),\n",
    "                  kernel_size=hp.Int(\"kernel_size\", min_value=min_kernel_size, max_value=max_kernel_size, step=kernel_size_step),\n",
    "                  nb_stacks=hp.Int(\"nb_stacks\", min_value=min_nb_stacks, max_value=max_nb_stacks, step=nb_stacks_step),\n",
    "                  dropout_rate=hp.Choice(\"dropout_rate\", values=[0.0, 0.01, 0.1, 0.2, 0.25, 0.35, 0.45, 0.5])\n",
    "                  ))\n",
    "    model.add(tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros))\n",
    "    model.add(tf.keras.layers.Reshape([OUT_STEPS, num_features]))\n",
    "\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4]),\n",
    "            # hp.Choice(\"amsgrad\", values=[True, False])\n",
    "        ),\n",
    "        metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "rows = []\n",
    "model_hyperopt_performances = pd.DataFrame(columns=[\"timestamp\", \"run\", \"model\", \"val_MAE\", \"MAE\", \"denormalized_MAE\",\n",
    "                                           \"denormalized_MASE\", \"denormalized_MAPE\", \"denormalized_sMAPE\", \"type\"])\n",
    "\n",
    "for i in range(hyperopt_runs):\n",
    "    # BayesianOptimization  options, see: https://keras.io/api/keras_tuner/tuners/bayesian/\n",
    "    tuner = BayesianOptimization (\n",
    "        build_model,\n",
    "        objective=\"val_loss\",\n",
    "        # factor=3,  # Integer, the reduction factor for the number of epochs and number of models for each bracket. Defaults to 3.\n",
    "        max_trials=max_trials,  # Integer, the total number of trials (model configurations) to test at most. Note that the oracle may interrupt the search before max_trial models have been tested if the search space has been exhausted.\n",
    "        seed=seed,  #  Optional integer, the random seed.\n",
    "        overwrite=True,\n",
    "        num_initial_points=3, # Optional number of randomly generated samples as initial training data for Bayesian optimization. If left unspecified, a value of 3 times the dimensionality of the hyperparameter space is used.\n",
    "        alpha=1e-4, # Float, the value added to the diagonal of the kernel matrix during fitting. It represents the expected amount of noise in the observed performances in Bayesian optimization. Defaults to 1e-4.\n",
    "        beta=2.6, # Float, the balancing factor of exploration and exploitation. The larger it is, the more explorative it is. Defaults to 2.6.\n",
    "        directory=\"keras_tuner_hyperopt\",\n",
    "        project_name=f\"{model_type}_hyperopt\",\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=stop_early_patience,\n",
    "                                                        restore_best_weights=True, mode='min')\n",
    "    # tuner.search(multi_window.train, epochs=MAX_EPOCHS, validation_data=multi_window.val, callbacks=[early_stopping])\n",
    "    tuner.search(multi_window.train, epochs=hyperopt_max_epochs, validation_data=multi_window.val, callbacks=[early_stopping])\n",
    "\n",
    "    # models = tuner.get_best_models(num_models=2)\n",
    "    best_model = tuner.get_best_models()[0]\n",
    "\n",
    "    tuner.results_summary()\n",
    "    hyperopt_model_name = f\"TCN_extended_hyperopt8_64\"\n",
    "    standardized_MAE, MAE, sMAPE, MAPE, MASE = evaluate_denormed_all(multi_window.test, multi_window.test_df, best_model)\n",
    "    new_row = {\"timestamp\": str(int(time.time())), \"run\": i, \"model\": hyperopt_model_name,\n",
    "        \"val_MAE\": best_model.evaluate(multi_window.val, verbose=0)[1],\n",
    "        \"MAE\": best_model.evaluate(multi_window.test, verbose=0)[1],\n",
    "        \"denormalized_MAE\": MAE,\n",
    "        \"denormalized_MASE\": MASE,\n",
    "        \"denormalized_MAPE\": MAPE,\n",
    "        \"denormalized_sMAPE\": sMAPE,\n",
    "        \"type\": model_type}\n",
    "    print(new_row)\n",
    "    model_hyperopt_performances = model_hyperopt_performances.append(new_row, ignore_index=True)\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "datetime_str = now.strftime(\"%m-%d-%Y_%H_%M_%S\")\n",
    "model_hyperopt_performances.to_csv(\"logs/{}_performances_{}_runs{}_{}.csv\"\n",
    "                      .format(hyperopt_model_name, model_type, hyperopt_runs, datetime_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## BO TCN-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from kerastuner import RandomSearch, Hyperband, BayesianOptimization\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "max_layers = 1\n",
    "min_units = 64 # 8 # 64\n",
    "max_units = 512 # 64 # 512\n",
    "units_step = 8\n",
    "stop_early_patience = 5\n",
    "hyperopt_max_epochs = 150 # 150 # 350 # 5 # 50 # DNN model iterations\n",
    "max_trials = 20 # 2 # 20 # 50  # BO Hyperopt max iterations\n",
    "hyperopt_runs = 8 # 8 # 1 # 10  # Number of times to repeat the hyperparameter search\n",
    "min_kernel_size = 1\n",
    "max_kernel_size = 48\n",
    "kernel_size_step = 1\n",
    "min_nb_stacks = 1\n",
    "max_nb_stacks = 3\n",
    "nb_stacks_step = 1\n",
    "\n",
    "def build_model(hp):\n",
    "\n",
    "    i = 0\n",
    "    # Model: TCN\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(TCN(hp.Int(\"units_\" + str(i), min_value=min_units, max_value=max_units, step=units_step),\n",
    "                  kernel_size=hp.Int(\"kernel_size\", min_value=min_kernel_size, max_value=max_kernel_size, step=kernel_size_step),\n",
    "                  #nb_stacks=hp.Int(\"nb_stacks\", min_value=min_nb_stacks, max_value=max_nb_stacks, step=nb_stacks_step),\n",
    "                  dropout_rate=hp.Choice(\"dropout_rate\", values=[0.0, 0.01, 0.1, 0.2, 0.25, 0.35, 0.45, 0.5])\n",
    "                  ))\n",
    "    model.add(tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros))\n",
    "    model.add(tf.keras.layers.Reshape([OUT_STEPS, num_features]))\n",
    "\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4]),\n",
    "            # hp.Choice(\"amsgrad\", values=[True, False])\n",
    "        ),\n",
    "        metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "rows = []\n",
    "model_hyperopt_performances = pd.DataFrame(columns=[\"timestamp\", \"run\", \"model\", \"val_MAE\", \"MAE\", \"denormalized_MAE\",\n",
    "                                           \"denormalized_MASE\", \"denormalized_MAPE\", \"denormalized_sMAPE\", \"type\"])\n",
    "\n",
    "for i in range(hyperopt_runs):\n",
    "    # BayesianOptimization  options, see: https://keras.io/api/keras_tuner/tuners/bayesian/\n",
    "    tuner = BayesianOptimization (\n",
    "        build_model,\n",
    "        objective=\"val_loss\",\n",
    "        # factor=3,  # Integer, the reduction factor for the number of epochs and number of models for each bracket. Defaults to 3.\n",
    "        max_trials=max_trials,  # Integer, the total number of trials (model configurations) to test at most. Note that the oracle may interrupt the search before max_trial models have been tested if the search space has been exhausted.\n",
    "        seed=seed,  #  Optional integer, the random seed.\n",
    "        overwrite=True,\n",
    "        num_initial_points=3, # Optional number of randomly generated samples as initial training data for Bayesian optimization. If left unspecified, a value of 3 times the dimensionality of the hyperparameter space is used.\n",
    "        alpha=1e-4, # Float, the value added to the diagonal of the kernel matrix during fitting. It represents the expected amount of noise in the observed performances in Bayesian optimization. Defaults to 1e-4.\n",
    "        beta=2.6, # Float, the balancing factor of exploration and exploitation. The larger it is, the more explorative it is. Defaults to 2.6.\n",
    "        directory=\"keras_tuner_hyperopt\",\n",
    "        project_name=f\"{model_type}_hyperopt\",\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=stop_early_patience,\n",
    "                                                        restore_best_weights=True, mode='min')\n",
    "    # tuner.search(multi_window.train, epochs=MAX_EPOCHS, validation_data=multi_window.val, callbacks=[early_stopping])\n",
    "    tuner.search(multi_window.train, epochs=hyperopt_max_epochs, validation_data=multi_window.val, callbacks=[early_stopping])\n",
    "\n",
    "    # models = tuner.get_best_models(num_models=2)\n",
    "    best_model = tuner.get_best_models()[0]\n",
    "\n",
    "    tuner.results_summary()\n",
    "    hyperopt_model_name = f\"TCN-specific_hyperopt64_512\"\n",
    "    standardized_MAE, MAE, sMAPE, MAPE, MASE = evaluate_denormed_all(multi_window.test, multi_window.test_df, best_model)\n",
    "    new_row = {\"timestamp\": str(int(time.time())), \"run\": i, \"model\": hyperopt_model_name,\n",
    "        \"val_MAE\": best_model.evaluate(multi_window.val, verbose=0)[1],\n",
    "        \"MAE\": best_model.evaluate(multi_window.test, verbose=0)[1],\n",
    "        \"denormalized_MAE\": MAE,\n",
    "        \"denormalized_MASE\": MASE,\n",
    "        \"denormalized_MAPE\": MAPE,\n",
    "        \"denormalized_sMAPE\": sMAPE,\n",
    "        \"type\": model_type}\n",
    "    print(new_row)\n",
    "    model_hyperopt_performances = model_hyperopt_performances.append(new_row, ignore_index=True)\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "datetime_str = now.strftime(\"%m-%d-%Y_%H_%M_%S\")\n",
    "model_hyperopt_performances.to_csv(\"logs/{}_performances_{}_runs{}_{}.csv\"\n",
    "                      .format(hyperopt_model_name, model_type, hyperopt_runs, datetime_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## BO GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from kerastuner import RandomSearch, Hyperband, BayesianOptimization\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "max_layers = 1\n",
    "min_units = 8 # 8 # 64\n",
    "max_units = 64 # 64 # 512\n",
    "units_step = 8\n",
    "stop_early_patience = 5\n",
    "hyperopt_max_epochs = 350 # 5 # 50 # DNN model iterations\n",
    "max_trials = 20 # 2 # 20 # 50  # BO Hyperopt max iterations\n",
    "hyperopt_runs = 10 # 1 # 10  # Number of times to repeat the hyperparameter search\n",
    "\n",
    "def build_model(hp):\n",
    "\n",
    "    i = 0\n",
    "    # Model: GRU\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.GRU(units=hp.Int(\"units_\" + str(i), min_value=min_units, max_value=max_units, step=units_step),\n",
    "                  dropout=hp.Choice(\"dropout\", values=[0.0, 0.01, 0.1, 0.2, 0.25, 0.35, 0.45, 0.5]),\n",
    "                    recurrent_dropout=hp.Choice(\"recurrent_dropout\", values=[0.0, 0.01, 0.1, 0.2, 0.25, 0.35, 0.45, 0.5])))\n",
    "    model.add(tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros))\n",
    "    model.add(tf.keras.layers.Reshape([OUT_STEPS, num_features]))\n",
    "\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4]),\n",
    "            # hp.Choice(\"amsgrad\", values=[True, False])\n",
    "        ),\n",
    "        metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "rows = []\n",
    "model_hyperopt_performances = pd.DataFrame(columns=[\"timestamp\", \"run\", \"model\", \"val_MAE\", \"MAE\", \"denormalized_MAE\",\n",
    "                                           \"denormalized_MASE\", \"denormalized_MAPE\", \"denormalized_sMAPE\", \"type\"])\n",
    "for i in range(hyperopt_runs):\n",
    "    # BayesianOptimization  options, see: https://keras.io/api/keras_tuner/tuners/bayesian/\n",
    "    tuner = BayesianOptimization (\n",
    "        build_model,\n",
    "        objective=\"val_loss\",\n",
    "        # factor=3,  # Integer, the reduction factor for the number of epochs and number of models for each bracket. Defaults to 3.\n",
    "        max_trials=max_trials,  # Integer, the total number of trials (model configurations) to test at most. Note that the oracle may interrupt the search before max_trial models have been tested if the search space has been exhausted.\n",
    "        seed=seed,  #  Optional integer, the random seed.\n",
    "        overwrite=True,\n",
    "        num_initial_points=3, # Optional number of randomly generated samples as initial training data for Bayesian optimization. If left unspecified, a value of 3 times the dimensionality of the hyperparameter space is used.\n",
    "        alpha=1e-4, # Float, the value added to the diagonal of the kernel matrix during fitting. It represents the expected amount of noise in the observed performances in Bayesian optimization. Defaults to 1e-4.\n",
    "        beta=2.6, # Float, the balancing factor of exploration and exploitation. The larger it is, the more explorative it is. Defaults to 2.6.\n",
    "        directory=\"keras_tuner_hyperopt\",\n",
    "        project_name=f\"{model_type}_hyperopt\",\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=stop_early_patience,\n",
    "                                                        restore_best_weights=True, mode='min')\n",
    "    # tuner.search(multi_window.train, epochs=MAX_EPOCHS, validation_data=multi_window.val, callbacks=[early_stopping])\n",
    "    tuner.search(multi_window.train, epochs=hyperopt_max_epochs, validation_data=multi_window.val, callbacks=[early_stopping])\n",
    "\n",
    "    # models = tuner.get_best_models(num_models=2)\n",
    "    best_model = tuner.get_best_models()[0]\n",
    "\n",
    "    tuner.results_summary()\n",
    "\n",
    "    hyperopt_model_name = f\"GRU_extended_hyperopt8_64\"\n",
    "    standardized_MAE, MAE, sMAPE, MAPE, MASE = evaluate_denormed_all(multi_window.test, multi_window.test_df, best_model)\n",
    "    new_row = {\"timestamp\": str(int(time.time())), \"run\": i, \"model\": hyperopt_model_name,\n",
    "        \"val_MAE\": best_model.evaluate(multi_window.val, verbose=0)[1],\n",
    "        \"MAE\": best_model.evaluate(multi_window.test, verbose=0)[1],\n",
    "        \"denormalized_MAE\": MAE,\n",
    "        \"denormalized_MASE\": MASE,\n",
    "        \"denormalized_MAPE\": MAPE,\n",
    "        \"denormalized_sMAPE\": sMAPE,\n",
    "        \"type\": model_type}\n",
    "    print(new_row)\n",
    "    model_hyperopt_performances = model_hyperopt_performances.append(new_row, ignore_index=True)\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "datetime_str = now.strftime(\"%m-%d-%Y_%H_%M_%S\")\n",
    "model_hyperopt_performances.to_csv(\"logs/{}_performances_{}_runs{}_{}.csv\"\n",
    "                      .format(hyperopt_model_name, model_type, hyperopt_runs, datetime_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Welch's t-test of unequal variances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "# Read CSV data:\n",
    "no_dr_model_runs = pd.read_csv(\"logs/model_performances_LSTM_GRU_TCN_attention_No DR_runs100_11-19-2021_15_03_55.csv\")\n",
    "dr_dnn_model_runs = pd.read_csv(\"logs/model_performances_LSTM_GRU_TCN_attention_DR-DNN_runs100_11-18-2021_04_27_01.csv\")\n",
    "\n",
    "print(\"Welch's t test:\\n\")\n",
    "for model_metric in [\"denormalized_MAE\", \"denormalized_MASE\", \"denormalized_MAPE\", \"denormalized_sMAPE\"]:\n",
    "    print(\"\\n{}:\".format(model_metric))\n",
    "    for model_subtype in [\"Linear\", \"Dense\", \"CNN\", \"LSTM\", \"GRU\", \"TCN\", \"LSTM Attention\"]:\n",
    "        k1 = dr_dnn_model_runs.loc[dr_dnn_model_runs.model == model_subtype][model_metric]\n",
    "        k2 = no_dr_model_runs.loc[no_dr_model_runs.model == model_subtype][model_metric]\n",
    "        l = stats.ttest_ind(k1, k2, equal_var=False) # alternative: { \"two-sided\", \"less\", \"greater\"}\n",
    "        bool_pass = True if l.statistic else False\n",
    "        print(\"{} - pass? {} statistic: {}, p-value: {} (DR-DNN mean: {}, No DR mean: {})\".format(model_subtype, bool_pass, round(l.statistic, 4), l.pvalue, round(k1.mean(), 2), round(k2.mean(), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "no_dr_model_runs = pd.read_csv(\"logs/model_performances_LSTM_GRU_TCN_attention_No DR_runs100_11-19-2021_15_03_55.csv\")\n",
    "dr_dnn_model_runs = pd.read_csv(\"logs/model_performances_LSTM_GRU_TCN_attention_DR-DNN_runs100_11-18-2021_04_27_01.csv\")\n",
    "dr_layer_time = 20 # seconds\n",
    "\n",
    "no_dr_model_runs[\"duration\"] =  no_dr_model_runs[\"timestamp\"].diff()\n",
    "dr_dnn_model_runs[\"duration\"] =  dr_dnn_model_runs[\"timestamp\"].diff()\n",
    "\n",
    "print(\"Model training times:\\n\")\n",
    "for model_subtype in [\"Linear\", \"Dense\", \"CNN\", \"LSTM\", \"GRU\", \"TCN\", \"LSTM Attention\"]:\n",
    "    print(\"\\n{}:\".format(model_subtype))\n",
    "    dr_dnn_model_times = dr_dnn_model_runs.loc[dr_dnn_model_runs.model == model_subtype][\"duration\"]\n",
    "    no_dr_model_times = no_dr_model_runs.loc[no_dr_model_runs.model == model_subtype][\"duration\"]\n",
    "    print(\"Training time increase: {}x - DR-DNN: {} +/- {} vs No DR: {} +/- {}\"\n",
    "          .format(round(dr_dnn_model_times.mean()/no_dr_model_times.mean(), 1), round(dr_dnn_model_times.mean(), 4),\n",
    "                  round(dr_dnn_model_times.std(), 3), round(no_dr_model_times.mean(), 3), round(no_dr_model_times.std(), 3)))\n",
    "    print(\"Training time increase (with DR layer): {}x - DR-DNN: DR layer = {} secs + {} +/- {} vs No DR: {} +/- {}\"\n",
    "              .format(round((dr_dnn_model_times.mean() + dr_layer_time)/no_dr_model_times.mean(), 1), dr_layer_time, round(dr_dnn_model_times.mean(), 4),\n",
    "                      round(dr_dnn_model_times.std(), 3), round(no_dr_model_times.mean(), 3), round(no_dr_model_times.std(), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "SNS boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model_runs = pd.read_csv(\"../modellingResultsDR_DNN_versus_MIMO.csv\")\n",
    "# model_runs = pd.read_csv(\"../modellingResultsDR_DNN_versus.csv\")\n",
    "# model_runs = pd.read_csv(\"model_performances_10runs.csv\")\n",
    "# model_runs = pd.read_csv(\"model_performances_100runs_old.csv\")\n",
    "model_runs = pd.read_csv(\"model_performances_100runs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run_count = 1 # 100 # 1\n",
    "# Set figure size\n",
    "sns.set( rc = {'figure.figsize' : ( 20, 20 ), 'axes.labelsize' : 12 })\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "g = sns.catplot(x=\"model\", y=\"MAE\",\n",
    "                hue=\"type\",\n",
    "                col=\"type\",\n",
    "                data=model_runs, kind=\"box\",\n",
    "                # height=4,\n",
    "                # aspect=.7\n",
    "                linewidth=1\n",
    "                )\n",
    "g.fig.suptitle(\"24h-ahead accuracy: DR-DNN vs DNN (train/val: 2017-20, test: 2020-21, runs: {})\".format(run_count) ,\n",
    "               fontsize = 'x-large' ,\n",
    "               fontweight = 'bold' )\n",
    "g.fig.subplots_adjust( top = 0.85 )\n",
    "g.set_axis_labels( \"Model\" , \"Mean Absolute Error (MAE)\", fontsize=16)\n",
    "g.set_xticklabels(rotation=45, fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "g.savefig('modellingResultsDR_DNN_versus.pdf', bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"DR-DNN\\nMean\", model_runs[model_runs[\"type\"] == \"DR-DNN\"].groupby([\"model\"])[\"MAE\"].mean().round(4))\n",
    "print(\"STD\", model_runs[model_runs[\"type\"] == \"DR-DNN\"].groupby([\"model\"])[\"MAE\"].std().round(4))\n",
    "print(\"\\nNo DR\\nMean\", model_runs[model_runs[\"type\"] == \"No DR\"].groupby([\"model\"])[\"MAE\"].mean().round(4))\n",
    "print(\"STD\", model_runs[model_runs[\"type\"] == \"No DR\"].groupby([\"model\"])[\"MAE\"].std().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model_runs = pd.read_csv(\"model_performances_100runs.csv\")\n",
    "model_runs = pd.read_csv(\"model_performances_DR-DNN_runs100_denormedMAE.csv\")\n",
    "\n",
    "run_count = 100\n",
    "# Set figure size\n",
    "sns.set( rc = {'figure.figsize' : ( 20, 24 ), 'axes.labelsize' : 14 })\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "g = sns.catplot(x=\"model\",\n",
    "                y=\"denormalized_MAE\",\n",
    "                # y=\"MAE\",\n",
    "                hue=\"type\",\n",
    "                col=\"type\",\n",
    "                data=model_runs, kind=\"box\",\n",
    "                # height=4,\n",
    "                # aspect=.7\n",
    "                linewidth=1\n",
    "                )\n",
    "g.fig.suptitle(\"24h-ahead accuracy: DR-DNN vs DNN (train/val: 2017-20, test: 2020-21, runs: {})\".format(run_count) ,\n",
    "               fontsize = 'x-large' ,\n",
    "               fontweight = 'bold' )\n",
    "g.fig.subplots_adjust( top = 0.85 )\n",
    "g.set_axis_labels( \"Model\" , \"Load (kW) Mean Absolute Error (MAE)\", fontsize=16)\n",
    "g.set_xticklabels(rotation=60, fontsize=18)\n",
    "\n",
    "plt.show()\n",
    "g.savefig('modellingResultsDR_DNN_versus_denormalized.pdf', bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model_runs = pd.read_csv(\"model_performances_100runs.csv\")\n",
    "model_runs = pd.read_csv(\"model_performances_DR-DNN_runs100_denormedMAE.csv\")\n",
    "run_count = 100\n",
    "\n",
    "sns.set()\n",
    "sns.set(\n",
    "    style=\"whitegrid\",\n",
    "    rc={\n",
    "    \"font.size\": 20,\n",
    "    'figure.figsize' : ( 30, 20),\n",
    "#     \"axes.titlesize\": 20,\n",
    "    \"axes.labelsize\": 20,\n",
    "#     \"legend.title_fontsize\": 20,\n",
    "#     \"axes.labelsize\": 20,\n",
    "#     \"legend.fontsize\": 20,\n",
    "#     \"font.size\": 20,\n",
    "#     \"xtick.labelsize\": 20,\n",
    "#     \"ytick.labelsize\": 20,\n",
    "#     \"legend.fontsize\": 20,\n",
    "#     \"legend.title_fontsize\": 20,\n",
    "})\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "# sns.set_context(\"paper\")\n",
    "\n",
    "plt.figure(figsize=(12,20))\n",
    "g = sns.catplot(x=\"model\",\n",
    "                y=\"denormalized_MAE\",\n",
    "                # y=\"MAE\",\n",
    "                # hue=\"type\",\n",
    "                col=\"type\",\n",
    "                data=model_runs,\n",
    "                kind=\"box\",  # box, boxen, bar, count, point, strip, swarm\n",
    "                height= 8.5, # 9,  # 8.5, 8,\n",
    "                aspect= 0.8, # 0.75,  # 0.8, 0.8\n",
    "                # col_wrap=2,\n",
    "                # n_boot=10,\n",
    "                linewidth=1,\n",
    "                # legend_out=False,\n",
    "                )\n",
    "g.fig.suptitle(\"24h-ahead accuracy: DR-DNN vs DNN (train/val: 2017-20, test: 2020-21, runs: {})\".format(run_count) ,\n",
    "               fontsize=24, fontweight='bold')\n",
    "# g.fig.title(fontsize=18)\n",
    "g.fig.subplots_adjust(top=0.90)\n",
    "g.set_axis_labels(\"Model\" , \"Load (kW) Mean Absolute Error (MAE)\", fontsize=22)\n",
    "g.set_xticklabels(rotation=50, fontsize=22)\n",
    "# g.set_yticklabels(rotation=90, fontsize=18)\n",
    "# g.add_legend()\n",
    "g.fig.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()\n",
    "g.savefig('modellingResultsDR_DNN_versus_denormalized.pdf', bbox_inches=\"tight\", dpi=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"DR-DNN\\nMean\", model_runs[model_runs[\"type\"] == \"DR-DNN\"].groupby([\"model\"])[\"denormalized_MAE\"].mean().round(4))\n",
    "print(\"STD\", model_runs[model_runs[\"type\"] == \"DR-DNN\"].groupby([\"model\"])[\"denormalized_MAE\"].std().round(4))\n",
    "print(\"\\nNo DR\\nMean\", model_runs[model_runs[\"type\"] == \"No DR\"].groupby([\"model\"])[\"denormalized_MAE\"].mean().round(4))\n",
    "print(\"STD\", model_runs[model_runs[\"type\"] == \"No DR\"].groupby([\"model\"])[\"denormalized_MAE\"].std().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Print latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_list = model_runs[model_runs[\"type\"] == \"DR-DNN\"].groupby([\"model\"])[\"MAE\"].mean().round(4).index.tolist()\n",
    "mean_DR_DNN = model_runs[model_runs[\"type\"] == \"DR-DNN\"].groupby([\"model\"])[\"MAE\"].mean().round(4).tolist()\n",
    "std_DR_DNN = model_runs[model_runs[\"type\"] == \"DR-DNN\"].groupby([\"model\"])[\"MAE\"].std().round(4).tolist()\n",
    "\n",
    "mean_No_DR = model_runs[model_runs[\"type\"] == \"No DR\"].groupby([\"model\"])[\"MAE\"].mean().round(4).tolist()\n",
    "std_No_DR = model_runs[model_runs[\"type\"] == \"No DR\"].groupby([\"model\"])[\"MAE\"].std().round(4).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "error_reductions = []\n",
    "for i in range(len(model_list)):\n",
    "    error_reduction = round((1 - ( min(mean_No_DR[i], mean_DR_DNN[i]) / max(mean_No_DR[i], mean_DR_DNN[i]))) * 100, 2)\n",
    "    if model_list[i] in [\"Linear\", \"Dense\", \"CNN\", \"LSTM\", \"GRU\"]:\n",
    "        error_reductions.append(error_reduction)\n",
    "    print(\"\\\\textit{{{}}} & {} $\\pm$ {} & {} $\\pm$ {} & {}\\% \\\\\\\\\".format(model_list[i], mean_No_DR[i], std_No_DR[i], mean_DR_DNN[i], std_DR_DNN[i], error_reduction))\n",
    "\n",
    "print(\"Overall error reduction: {}%\".format(round(np.mean(np.array(error_reductions)), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k = model_runs[model_runs[\"type\"] == \"DR-DNN\"].groupby([\"model\"])[\"MAE\"].mean().round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g.savefig(\"modellingResultsDR_DNN_versus.png\")\n",
    "g.savefig(\"modellingResultsDR_DNN_versus.svg\")\n",
    "g.savefig(\"modellingResultsDR_DNN_versus.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The gains achieved going from a dense model to convolutional and recurrent models are only a few percent (if any), and the autoregressive model performed clearly worse. So these more complex approaches may not be worth while on **this** problem, but there was no way to know without trying, and these models could be helpful for **your** problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Next steps\n",
    "\n",
    "This tutorial was a quick introduction to time series forecasting using TensorFlow.\n",
    "\n",
    "* For further understanding, see:\n",
    "  * Chapter 15 of [Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), 2nd Edition\n",
    "  * Chapter 6 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python).\n",
    "  * Lesson 8 of [Udacity's intro to TensorFlow for deep learning](https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187), and the [exercise notebooks](https://github.com/tensorflow/examples/tree/master/courses/udacity_intro_to_tensorflow_for_deep_learning)\n",
    "* Also remember that you can implement any [classical time series model](https://otexts.com/fpp2/index.html) in TensorFlow, this tutorial just focuses on TensorFlow's built-in functionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Predict last day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# best_model = multi_lstm_model\n",
    "best_model = multi_gru_model\n",
    "# best_model = hypermodel\n",
    "\n",
    "predictions = best_model.predict(multi_window.test)\n",
    "final_step = predictions.shape[0] - 1\n",
    "hour = 23\n",
    "var = 0\n",
    "print(predictions[final_step, hour, var].shape)\n",
    "print(predictions[final_step, hour, var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "denormed_predictions = (predictions[final_step, :, var] * train_std[0]) + train_mean[0]\n",
    "plt.plot(denormed_predictions)\n",
    "plt.title(\"Forecast of {} {}\".format(test_df.tail(1).index.month_name()[0], test_df.tail(1).index.day.values[0] + 1))\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Load (kW)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### TODO: Custom test data evaluate method:\n",
    "1. de_norm (above)\n",
    "2. DR + de_norm foreach single step of TEST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_model = multi_gru_model\n",
    "# best_model = multi_lstm_model\n",
    "predictions = best_model.predict(multi_window.test)\n",
    "all_preds = []\n",
    "normed_preds = []\n",
    "test_count = test_df.shape[0] - 1\n",
    "\n",
    "for i in range(predictions.shape[0] - 1):\n",
    "    denormed_prediction = (predictions[i, :, var] * train_std[0]) + train_mean[0]\n",
    "    all_preds.append(denormed_prediction)\n",
    "    normed_prediction = predictions[i, :, var]\n",
    "    normed_preds.append(normed_prediction)\n",
    "\n",
    "# expected_test = df[int(n*(train_ratio + validation_ratio)):][\"Load (kW)\"].values\n",
    "expected_test = df[\"Load (kW)\"].tail(test_count).values\n",
    "\n",
    "denormed_predictions = np.array(all_preds).flatten()\n",
    "denormed_predictions_test = denormed_predictions[-test_count:]\n",
    "\n",
    "normed_predictions = np.array(normed_preds).flatten()\n",
    "normed_predictions_test = normed_predictions[-test_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = best_model.evaluate(multi_window.val)\n",
    "dict(zip(best_model.metrics_names, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = best_model.evaluate(multi_window.train)\n",
    "dict(zip(best_model.metrics_names, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = best_model.evaluate(multi_window.test)\n",
    "dict(zip(best_model.metrics_names, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(expected_test - denormed_predictions_test))\n",
    "print(f\"MAE: {mae}\")\n",
    "smape = 100 * np.mean(np.abs(denormed_predictions_test - expected_test)/\n",
    "                      ((np.abs(expected_test) + np.abs(denormed_predictions_test))/ 2.0))\n",
    "print(f\"sMAPE: {np.round(smape, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normed_expected_test = test_df[\"Load (kW)\"].tail(test_count).values\n",
    "\n",
    "mae = np.mean(np.abs(normed_expected_test - normed_predictions_test))\n",
    "print(f\"MAE: {mae}\")\n",
    "smape = 100 * np.mean(np.abs(normed_predictions_test - normed_expected_test)/\n",
    "                      ((np.abs(normed_expected_test) + np.abs(normed_predictions_test))/ 2.0))\n",
    "print(f\"sMAPE: {np.round(smape, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions_lag7d = df[\"Load (kW)_lag7d\"].tail(test_count).values\n",
    "mae = np.mean(np.abs(expected_test - predictions_lag7d))\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "smape = 100 * np.mean(np.abs(predictions_lag7d - expected_test)/\n",
    "                      ((np.abs(expected_test) + np.abs(predictions_lag7d))/ 2.0))\n",
    "print(f\"sMAPE: {np.round(smape, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = best_model.evaluate(multi_window.test)\n",
    "dict(zip(best_model.metrics_names, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t1 = test_df[\"Load (kW)\"].values\n",
    "t2 = t1[1:]\n",
    "\n",
    "mae = np.mean(np.abs(t2 - normed_predictions_test))\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "smape = 100 * np.mean(np.abs(normed_predictions_test - t2)/\n",
    "                      ((np.abs(t2) + np.abs(normed_predictions_test))/ 2.0))\n",
    "print(f\"sMAPE: {np.round(smape, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t1 = (test_df[\"Load (kW)_lag7d\"] * train_std[0]) + train_mean[0]\n",
    "t2 = t1.values[1:]\n",
    "\n",
    "mae = np.mean(np.abs(t2 - denormed_predictions_test))\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "smape = 100 * np.mean(np.abs(denormed_predictions_test - t2)/\n",
    "                      ((np.abs(t2) + np.abs(denormed_predictions_test))/ 2.0))\n",
    "print(f\"sMAPE: {np.round(smape, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(test_df[\"Load (kW)\"] - test_df['Load (kW)'].shift(24*7)))\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "smape = 100 * np.mean(np.abs(test_df['Load (kW)'].shift(24*7) - test_df[\"Load (kW)\"])/\n",
    "                      ((np.abs(test_df[\"Load (kW)\"]) + np.abs(test_df['Load (kW)'].shift(24*7)))/ 2.0))\n",
    "print(f\"sMAPE: {np.round(smape, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(test_df[\"Load (kW)\"] - test_df[\"Load (kW)_lag7d\"]))\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "smape = 100 * np.mean(np.abs(test_df[\"Load (kW)_lag7d\"] - test_df[\"Load (kW)\"])/\n",
    "                      ((np.abs(test_df[\"Load (kW)\"]) + np.abs(test_df[\"Load (kW)_lag7d\"]))/ 2.0))\n",
    "print(f\"sMAPE: {np.round(smape, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t1 = (test_df[\"Load (kW)_lag7d\"] * train_std[0]) + train_mean[0]\n",
    "t2 = t1.values[1:]\n",
    "mae = np.mean(np.abs(expected_test - t2))\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "smape = 100 * np.mean(np.abs(t2 - expected_test)/\n",
    "                      ((np.abs(expected_test) + np.abs(t2))/ 2.0))\n",
    "print(f\"sMAPE: {np.round(smape, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Load (kW)\"].tail(140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "expected_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# np_all_preds = np.array(all_preds)\n",
    "plt.plot(predictions_lag7d)\n",
    "plt.plot(expected_test)\n",
    "plt.title(\"Expected vs Predicted\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Load (kW)\")\n",
    "plt.legend([\"Expected\", \"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# np_all_preds = np.array(all_preds)\n",
    "# plt.plot(denormed_predictions_test)\n",
    "plt.plot(expected_test)\n",
    "plt.title(\"Expected vs Predicted\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Load (kW)\")\n",
    "plt.legend([\"Expected\", \"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(multi_window.test_df.shape)\n",
    "multi_window.test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_window.test_df.head(3200).tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_window.test_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_model.evaluate(multi_window.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_denormed(data, df, model, batch_size=32):\n",
    "    absolute_error = []\n",
    "    denormed_absolute_error = []\n",
    "\n",
    "    for batch_inputs, batch_labels in data.take(int(df.shape[0]/batch_size)):\n",
    "\n",
    "        prediction = model.predict(batch_inputs)\n",
    "        residuals = tf.abs(batch_labels - prediction)\n",
    "        absolute_error += residuals.numpy().flatten().tolist()\n",
    "\n",
    "        denormed_prediction = (prediction * train_std[0]) + train_mean[0]\n",
    "        denormed_expected = (batch_labels * train_std[0]) + train_mean[0]\n",
    "        denormed_residuals = tf.abs(denormed_expected - denormed_prediction)\n",
    "        denormed_absolute_error += denormed_residuals.numpy().flatten().tolist()\n",
    "\n",
    "    return np.array(absolute_error).mean(), np.array(denormed_absolute_error).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_model = multi_lstm_model\n",
    "# best_model = multi_dense_model\n",
    "# best_model = multi_gru_model\n",
    "mae, denormed_mae = evaluate_denormed(multi_window.test, multi_window.test_df, best_model)\n",
    "print(f\"mae: {mae}\")\n",
    "print(f\"denormed mae: {denormed_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_model.evaluate(multi_window.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "do_save = True\n",
    "date_name_string = \"{}-{}-Theodorakos-Konstantinos.csv\".format(test_df.tail(1).index.month_name()[0], test_df.tail(1).index.day.values[0] + 1)\n",
    "print(date_name_string)\n",
    "if do_save:\n",
    "    np.savetxt(date_name_string, denormed_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create Multi-Output Regression Model (see: https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/model_agnostic/Multioutput%20Regression%20SHAP.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Multiwindow train Df -> X, y numpy of 32 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "IN_STEPS = 24\n",
    "OUT_STEPS = 24\n",
    "input_features = train_df.shape[1]\n",
    "iter = 0\n",
    "for example_inputs, example_labels in multi_window.train.take(4):\n",
    "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "  print(f'Labels shape (batch, time, features): {example_labels.shape}')\n",
    "  print(type(example_labels))\n",
    "  # print(example_labels.numpy()[0])\n",
    "\n",
    "  y_0 = example_labels.numpy()\n",
    "  y = np.reshape(y_0, (batch_size, OUT_STEPS))\n",
    "\n",
    "  X_0 = example_inputs.numpy()\n",
    "  print(\"X_0\", X_0.shape)\n",
    "  X = np.reshape(X_0, (batch_size, IN_STEPS * input_features))\n",
    "  print(X.shape)\n",
    "\n",
    "  if iter == 0:\n",
    "      Ys = y.copy()\n",
    "      Xs = X.copy()\n",
    "      iter +=1\n",
    "  else:\n",
    "    Ys = np.vstack((Ys, y))\n",
    "    Xs = np.vstack((Xs, X))\n",
    "  print(y.shape)\n",
    "print()\n",
    "print(Ys.shape)\n",
    "print()\n",
    "print(Xs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## TODO: 32 -> 100+ stacking of batch examples?\n",
    "## TODO: x 3d -> 2d columns are correct in order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_columns = multi_window.train_df.columns[1:]\n",
    "y_columns = multi_window.train_df.columns[0]\n",
    "print(\"X columns:\", X_columns)\n",
    "print(\"y columns:\", y_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_features = train_df.shape[1]\n",
    "out_features = 1\n",
    "n_samples = 32\n",
    "IN_STEPS = 24\n",
    "OUT_STEPS = 24\n",
    "X, y = make_regression(n_samples=n_samples, n_features=IN_STEPS * input_features,\n",
    "                     n_informative=int(0.7 * IN_STEPS * input_features),\n",
    "                     n_targets=OUT_STEPS * out_features, random_state=0)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# SHAPley  model agnostic: IEEE Elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset0(n_samples=100, in_steps=24, input_features=1, out_steps=24, out_features=1):\n",
    "  # print(\"n_samples {}, n_features {}, n_targets {}\".format(n_samples, n_features, n_targets))\n",
    "  X, y = make_regression(n_samples=n_samples, n_features=in_steps * input_features,\n",
    "                         n_informative=int(0.7 * in_steps * input_features),\n",
    "                         n_targets=out_steps * out_features, random_state=0)\n",
    "  feature_cols = []\n",
    "  label_cols = []\n",
    "  for i in range(in_steps):\n",
    "      for j in range(input_features):\n",
    "        feature_cols.append(\"feature_{}h_var{}\".format(i, j))\n",
    "  for i in range(out_steps):\n",
    "      for j in range(out_features):\n",
    "        label_cols.append(\"labels_{}h_var{}\".format(i, j))\n",
    "  df_features = pd.DataFrame(data = X, columns = feature_cols)\n",
    "  df_labels = pd.DataFrame(data = y, columns = label_cols)\n",
    "  return df_features, df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset1(n_samples=32, in_steps=24, input_features=1, out_steps=24, out_features=1):\n",
    "\n",
    "    for batch_inputs, batch_labels in multi_window.train.take(1):\n",
    "        X = np.reshape(batch_inputs.numpy(), (batch_size, IN_STEPS * input_features))\n",
    "        y = np.reshape(batch_labels.numpy(), (batch_size, OUT_STEPS))\n",
    "\n",
    "    feature_cols = []\n",
    "    label_cols = []\n",
    "    for i in range(in_steps):\n",
    "        for j in range(input_features):\n",
    "            feature_cols.append(\"{}-{}h\".format(multi_window.train_df.columns[j], i))\n",
    "    for i in range(out_steps):\n",
    "        for j in range(out_features):\n",
    "            label_cols.append(\"{}-{}h\".format(multi_window.train_df.columns[0], i + in_steps))\n",
    "    df_features = pd.DataFrame(data = X, columns = feature_cols)\n",
    "    df_labels = pd.DataFrame(data = y, columns = label_cols)\n",
    "    return df_features, df_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TODO: also return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: no hardcoded dataset input\n",
    "def get_dataset(dataset, dataset_df, n_samples=32, in_steps=24, input_features=1, out_steps=24, out_features=1):\n",
    "\n",
    "    iter = 0\n",
    "    for batch_inputs, batch_labels in dataset.take(int(n_samples/batch_size)):\n",
    "    # for batch_inputs, batch_labels in multi_window.train:\n",
    "        X = np.reshape(batch_inputs.numpy(), (batch_size, IN_STEPS * input_features))\n",
    "        y = np.reshape(batch_labels.numpy(), (batch_size, OUT_STEPS))\n",
    "        if iter == 0:\n",
    "            Ys = y.copy()\n",
    "            Xs = X.copy()\n",
    "            iter +=1\n",
    "        else:\n",
    "            Ys = np.vstack((Ys, y))\n",
    "            Xs = np.vstack((Xs, X))\n",
    "\n",
    "    feature_cols = []\n",
    "    label_cols = []\n",
    "    for i in range(in_steps):\n",
    "        for j in range(input_features):\n",
    "            feature_cols.append(\"{}-{}h\".format(dataset_df.columns[j], i))\n",
    "    for i in range(out_steps):\n",
    "        for j in range(out_features):\n",
    "            label_cols.append(\"{}-{}h\".format(dataset_df.columns[0], i + in_steps))\n",
    "    df_features = pd.DataFrame(data=Xs, columns=feature_cols)\n",
    "    df_labels = pd.DataFrame(data=Ys, columns=label_cols)\n",
    "    # TODO: also return dates...\n",
    "    date_indices = dataset_df.index[0:int(n_samples/batch_size) * batch_size]\n",
    "\n",
    "    return df_features, df_labels, date_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data\n",
    "## IN (None, 24, 36) -> (None, 768)\n",
    "## OUT (None, 24, 1) -> (None 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## TODO: Residual AutoCorr and CrossCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X, y, date_indices = get_dataset(multi_window.train, multi_window.train_df, n_samples=batch_size * 100, in_steps=IN_STEPS, input_features=input_features,\n",
    "                    out_steps=OUT_STEPS, out_features=out_features)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(date_indices.shape)\n",
    "print(\"X columns:\", X.columns)\n",
    "print(\"y columns:\", y.columns)\n",
    "print(\"date_indices[0]:\", date_indices[0])\n",
    "print(\"date_indices[-1]:\", date_indices[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test, y_test, date_indices_test = get_dataset(multi_window.test, multi_window.test_df, n_samples=batch_size * 100, in_steps=IN_STEPS, input_features=input_features,\n",
    "                    out_steps=OUT_STEPS, out_features=out_features)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(date_indices_test.shape)\n",
    "print(\"X columns:\", X_test.columns)\n",
    "print(\"y columns:\", y_test.columns)\n",
    "print(\"date_indices[0]:\", date_indices_test[0])\n",
    "print(\"date_indices[-1]:\", date_indices_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_val, y_val, date_indices_val = get_dataset(multi_window.val, multi_window.val_df, n_samples=batch_size * 100, in_steps=IN_STEPS, input_features=input_features,\n",
    "                    out_steps=OUT_STEPS, out_features=out_features)\n",
    "\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(date_indices_val.shape)\n",
    "print(\"X columns:\", X_val.columns)\n",
    "print(\"y columns:\", y_val.columns)\n",
    "print(\"date_indices[0]:\", date_indices_val[0])\n",
    "print(\"date_indices[-1]:\", date_indices_val[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(in_steps=24, input_features=1, out_steps=24, out_features=1):\n",
    "\n",
    "    multi_lstm_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "        tf.keras.layers.Dense(OUT_STEPS*out_features,\n",
    "                              kernel_initializer=tf.initializers.zeros),\n",
    "        tf.keras.layers.Reshape([OUT_STEPS, out_features])\n",
    "    ])\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Reshape([in_steps, input_features, ]),\n",
    "        multi_lstm_model,\n",
    "        tf.keras.layers.Reshape([OUT_STEPS,])\n",
    "    ])\n",
    "\n",
    "    # model = tf.keras.Sequential([  # TODO: Backup\n",
    "    #     tf.keras.layers.Reshape([in_steps, input_features, ]),\n",
    "    #     tf.keras.layers.LSTM(16, return_sequences=False),\n",
    "    #     tf.keras.layers.Dense(out_steps * out_features, kernel_initializer='he_uniform'),\n",
    "    # ])\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # model.compile(loss='mae', optimizer='nadam')\n",
    "    return model\n",
    "\n",
    "input_features = train_df.shape[1]\n",
    "out_features = 1\n",
    "n_samples = batch_size * 400\n",
    "IN_STEPS = 24\n",
    "OUT_STEPS = 24\n",
    "\n",
    "# X, y = get_dataset0(n_samples=n_samples, in_steps=IN_STEPS, input_features=input_features,\n",
    "#                    out_steps=OUT_STEPS, out_features=out_features)\n",
    "X, y, date_indices = get_dataset(multi_window.train, multi_window.train_df, n_samples=n_samples, in_steps=IN_STEPS, input_features=input_features,\n",
    "                    out_steps=OUT_STEPS, out_features=out_features)\n",
    "\n",
    "n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "print(\"X.shape: {}, y.shape: {}\".format(X.shape, y.shape))\n",
    "model = get_model(in_steps=IN_STEPS, input_features=input_features, out_steps=OUT_STEPS,\n",
    "                  out_features=out_features)\n",
    "model.fit(X, y, verbose=0, epochs=100)\n",
    "print(\"Eval accuracy MAE: {}\".format(model.evaluate(x=X, y=y)))\n",
    "model.predict(X.iloc[0:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install shap\n",
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "head_samples = 50 * 1 # 50\n",
    "shap_nsamples = 100 * 4 # 100\n",
    "\n",
    "explainer = shap.KernelExplainer(model = model.predict, data = X.head(head_samples), link = \"identity\")\n",
    "\n",
    "# Set the index of the specific example to explain\n",
    "X_idx = 0\n",
    "\n",
    "shap_value_single = explainer.shap_values(X = X.iloc[X_idx:X_idx+1,:], nsamples = shap_nsamples)\n",
    "\n",
    "X.iloc[X_idx:X_idx+1,:]\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Create the list of all labels for the drop down list\n",
    "list_of_labels = y.columns.to_list()\n",
    "# Create a list of tuples so that the index of the label is what is returned\n",
    "tuple_of_labels = list(zip(list_of_labels, range(len(list_of_labels))))\n",
    "# Create a widget for the labels and then display the widget\n",
    "current_label = widgets.Dropdown(options=tuple_of_labels, value=0, description='Select Label:')\n",
    "\n",
    "# Display the dropdown list (Note: access index value with 'current_label.value')\n",
    "current_label\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "print(f'Current label Shown: {list_of_labels[current_label.value]}')\n",
    "fig_force_plot = shap.force_plot(base_value = explainer.expected_value[current_label.value],\n",
    "                shap_values = shap_value_single[current_label.value],\n",
    "                features = X.iloc[X_idx:X_idx+1,:], show=False)\n",
    "plt.savefig('force_plot.png')\n",
    "plt.savefig('force_plot.svg')\n",
    "\n",
    "# Note: We are limiting to the first 50 training examples since it takes time to calculate the full number of samples\n",
    "shap_values = explainer.shap_values(X = X.iloc[0:head_samples,:], nsamples=shap_nsamples)\n",
    "\n",
    "shap.initjs()\n",
    "print(f'Current Label Shown: {list_of_labels[current_label.value]}\\n')\n",
    "fig_summary_plot = shap.summary_plot(shap_values = shap_values[current_label.value], features = X.iloc[0:head_samples,:])\n",
    "plt.savefig('summary_plot.png')\n",
    "plt.savefig('summary_plot.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# TODO: De-normalize \"labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1h ahead feature importance (9 morning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hour_ahead = 0\n",
    "shap.initjs()\n",
    "current_label = widgets.Dropdown(options=tuple_of_labels, value=hour_ahead, description='Select Label:')\n",
    "plt.title('Feature impact on {}h-ahead Load (kW) forecast (8 morning)'.format(hour_ahead + 1))\n",
    "print(f'Current Label Shown: {list_of_labels[current_label.value]}\\n')\n",
    "fig_summary_plot = shap.summary_plot(shap_values = shap_values[current_label.value], features = X.iloc[0:head_samples,:], show=False)\n",
    "plt.savefig('summary_plot_{}h_ahead.png'.format(hour_ahead + 1), bbox_inches=\"tight\", dpi=600)\n",
    "plt.savefig('summary_plot_{}h_ahead.svg'.format(hour_ahead + 1), bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "6h ahead feature importance (14:00):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hour_ahead = 5\n",
    "shap.initjs()\n",
    "current_label = widgets.Dropdown(options=tuple_of_labels, value=hour_ahead, description='Select Label:')\n",
    "plt.title('Feature impact on {}h-ahead Load (kW) forecast (13:00)'.format(hour_ahead + 1))\n",
    "print(f'Current Label Shown: {list_of_labels[current_label.value]}\\n')\n",
    "fig_summary_plot = shap.summary_plot(shap_values = shap_values[current_label.value], features = X.iloc[0:head_samples,:], show=False)\n",
    "plt.savefig('summary_plot_{}h_ahead.png'.format(hour_ahead + 1), bbox_inches=\"tight\", dpi=600)\n",
    "plt.savefig('summary_plot_{}h_ahead.svg'.format(hour_ahead + 1), bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "12h ahead feature importance (20:00):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hour_ahead = 11\n",
    "shap.initjs()\n",
    "current_label = widgets.Dropdown(options=tuple_of_labels, value=hour_ahead, description='Select Label:')\n",
    "plt.title('Feature impact on {}h-ahead Load (kW) forecast (19:00)'.format(hour_ahead + 1))\n",
    "print(f'Current Label Shown: {list_of_labels[current_label.value]}\\n')\n",
    "fig_summary_plot = shap.summary_plot(shap_values = shap_values[current_label.value], features = X.iloc[0:head_samples,:], show=False)\n",
    "plt.savefig('summary_plot_{}h_ahead.png'.format(hour_ahead + 1), bbox_inches=\"tight\", dpi=600)\n",
    "plt.savefig('summary_plot_{}h_ahead.svg'.format(hour_ahead + 1), bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "18h ahead feature importance (2 at night):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hour_ahead = 17\n",
    "current_label = widgets.Dropdown(options=tuple_of_labels, value=hour_ahead, description='Select Label:')\n",
    "plt.title('Feature impact on {}h-ahead Load (kW) forecast (1 at night)'.format(hour_ahead + 1))\n",
    "print(f'Current Label Shown: {list_of_labels[current_label.value]}\\n')\n",
    "fig_summary_plot = shap.summary_plot(shap_values = shap_values[current_label.value], features = X.iloc[0:head_samples,:], show=False)\n",
    "plt.savefig('summary_plot_{}h_ahead.png'.format(hour_ahead + 1), bbox_inches=\"tight\", dpi=600)\n",
    "plt.savefig('summary_plot_{}h_ahead.svg'.format(hour_ahead + 1), bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "24h ahead feature importance (8 morning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hour_ahead = 23\n",
    "shap.initjs()\n",
    "current_label = widgets.Dropdown(options=tuple_of_labels, value=hour_ahead, description='Select Label:')\n",
    "plt.title('Feature impact on {}h-ahead Load (kW) forecast (7 morning)'.format(hour_ahead + 1))\n",
    "print(f'Current Label Shown: {list_of_labels[current_label.value]}\\n')\n",
    "fig_summary_plot = shap.summary_plot(shap_values = shap_values[current_label.value], features = X.iloc[0:head_samples,:], show=False)\n",
    "plt.savefig('summary_plot_{}h_ahead.png'.format(hour_ahead + 1), bbox_inches=\"tight\", dpi=600)\n",
    "plt.savefig('summary_plot_{}h_ahead.svg'.format(hour_ahead + 1), bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print the JS visualization code to the notebook\n",
    "shap.initjs()\n",
    "\n",
    "print(f'Current label Shown: {list_of_labels[current_label.value]}')\n",
    "\n",
    "fig_force_plot2 = shap.force_plot(base_value = explainer.expected_value[current_label.value],\n",
    "                shap_values = shap_value_single[current_label.value],\n",
    "                features = X.iloc[X_idx:X_idx+1,:], show=False)\n",
    "plt.savefig('force_plot2.png')\n",
    "plt.savefig('force_plot2.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm as pl\n",
    "\n",
    "display_count = 18\n",
    "shap_legend = [\"1h-ahead\", \"2h-ahead\", \"3h-ahead\", \"4h-ahead\", \"5h-ahead\", \"6h-ahead\", \"7h-ahead\", \"8h-ahead\", \"9h-ahead\", \"10h-ahead\", \"11h-ahead\", \"12h-ahead\", \"13h-ahead\", \"14h-ahead\", \"15h-ahead\", \"16h-ahead\", \"17h-ahead\", \"18h-ahead\", \"19h-ahead\", \"20h-ahead\", \"21h-ahead\", \"22h-ahead\", \"23h-ahead\", \"24h-ahead\", ]\n",
    "dataset_label = \"train\"\n",
    "plt.title('Feature impact on hour-ahead Load (kW) forecast ({}, top {} from {}x{} vars)'.format(dataset_label, display_count, int(X.shape[1]/24), 24))\n",
    "shap.summary_plot(shap_values, X, show=False, class_names=shap_legend,\n",
    "    color=pl.get_cmap(\"tab20b\", 24), max_display=display_count)\n",
    "plt.savefig('summary_plot_all_{}_seed{}.pdf'.format(dataset_label, seed), bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display_count = 25\n",
    "shap_legend = [\"1h-ahead\", \"2h-ahead\", \"3h-ahead\", \"4h-ahead\", \"5h-ahead\", \"6h-ahead\", \"7h-ahead\", \"8h-ahead\", \"9h-ahead\", \"10h-ahead\", \"11h-ahead\", \"12h-ahead\", \"13h-ahead\", \"14h-ahead\", \"15h-ahead\", \"16h-ahead\", \"17h-ahead\", \"18h-ahead\", \"19h-ahead\", \"20h-ahead\", \"21h-ahead\", \"22h-ahead\", \"23h-ahead\", \"24h-ahead\", ]\n",
    "dataset_label = \"train\"\n",
    "plt.title('Feature impact on hour-ahead Load (kW) forecast ({}, top {} from {}x{} vars)'.format(dataset_label, display_count, int(X.shape[1]/24), 24))\n",
    "shap.summary_plot(shap_values, X, show=False, class_names=shap_legend,\n",
    "    color=pl.get_cmap(\"tab20b\", 24), max_display=display_count)\n",
    "plt.savefig('summary_plot_all_{}_seed{}_count{}.pdf'.format(dataset_label, seed, display_count), bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display_count = 40\n",
    "shap_legend = [\"1h-ahead\", \"2h-ahead\", \"3h-ahead\", \"4h-ahead\", \"5h-ahead\", \"6h-ahead\", \"7h-ahead\", \"8h-ahead\", \"9h-ahead\", \"10h-ahead\", \"11h-ahead\", \"12h-ahead\", \"13h-ahead\", \"14h-ahead\", \"15h-ahead\", \"16h-ahead\", \"17h-ahead\", \"18h-ahead\", \"19h-ahead\", \"20h-ahead\", \"21h-ahead\", \"22h-ahead\", \"23h-ahead\", \"24h-ahead\", ]\n",
    "dataset_label = \"train\"\n",
    "plt.title('Feature impact on hour-ahead Load (kW) forecast ({}, top {} from {}x{} vars)'.format(dataset_label, display_count, int(X.shape[1]/24), 24))\n",
    "shap.summary_plot(shap_values, X, show=False, class_names=shap_legend,\n",
    "    color=pl.get_cmap(\"tab20b\", 24), max_display=display_count)\n",
    "plt.savefig('summary_plot_all_{}_seed{}_count{}.pdf'.format(dataset_label, seed, display_count), bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display_count = 50\n",
    "shap_legend = [\"1h-ahead\", \"2h-ahead\", \"3h-ahead\", \"4h-ahead\", \"5h-ahead\", \"6h-ahead\", \"7h-ahead\", \"8h-ahead\", \"9h-ahead\", \"10h-ahead\", \"11h-ahead\", \"12h-ahead\", \"13h-ahead\", \"14h-ahead\", \"15h-ahead\", \"16h-ahead\", \"17h-ahead\", \"18h-ahead\", \"19h-ahead\", \"20h-ahead\", \"21h-ahead\", \"22h-ahead\", \"23h-ahead\", \"24h-ahead\", ]\n",
    "dataset_label = \"train\"\n",
    "plt.title('Feature impact on hour-ahead Load (kW) forecast ({}, top {} from {}x{} vars)'.format(dataset_label, display_count, int(X.shape[1]/24), 24))\n",
    "shap.summary_plot(shap_values, X, show=False, class_names=shap_legend,\n",
    "    color=pl.get_cmap(\"tab20b\", 24), max_display=display_count)\n",
    "plt.savefig('summary_plot_all_{}_seed{}_count{}.pdf'.format(dataset_label, seed, display_count), bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as pl\n",
    "#\n",
    "# # print('shap_legend = [', end=\"\")\n",
    "# # # for i in range(24):\n",
    "# # for i in [13, 17, 18, 14, 23, 12, 20, 9, 6, 11, 7, 15, 5, 8, 16, 19, 21, 10, 2, 4, 1, 3, 22, 0]:\n",
    "# #     print('\"{}h-ahead\"'.format(i+1),end=\", \")\n",
    "# # print(']', end=\"\")\n",
    "#\n",
    "# display_count = 20\n",
    "# shap_legend = [\"1h-ahead\", \"2h-ahead\", \"3h-ahead\", \"4h-ahead\", \"5h-ahead\", \"6h-ahead\", \"7h-ahead\", \"8h-ahead\", \"9h-ahead\", \"10h-ahead\", \"11h-ahead\", \"12h-ahead\", \"13h-ahead\", \"14h-ahead\", \"15h-ahead\", \"16h-ahead\", \"17h-ahead\", \"18h-ahead\", \"19h-ahead\", \"20h-ahead\", \"21h-ahead\", \"22h-ahead\", \"23h-ahead\", \"24h-ahead\", ]\n",
    "# # shap_legend = [\"14h-ahead\", \"18h-ahead\", \"19h-ahead\", \"15h-ahead\", \"24h-ahead\", \"13h-ahead\", \"21h-ahead\", \"10h-ahead\", \"7h-ahead\", \"12h-ahead\", \"8h-ahead\", \"16h-ahead\", \"6h-ahead\", \"9h-ahead\", \"17h-ahead\", \"20h-ahead\", \"22h-ahead\", \"11h-ahead\", \"3h-ahead\", \"5h-ahead\", \"2h-ahead\", \"4h-ahead\", \"23h-ahead\", \"1h-ahead\", ]\n",
    "#\n",
    "# dataset_label = \"train\"\n",
    "# plt.title('Summary plot: Feature impact on day-ahead Load (kW) forecast ({} data)'.format(dataset_label))\n",
    "# shap.summary_plot(\n",
    "#     shap_values, X, show=False, class_names=shap_legend,\n",
    "#     color=pl.get_cmap(\n",
    "#     #     # \"jet\", # rainbow, turbo, jet:\n",
    "#         \"tab20b\", # Qualitative color maps: bwr, hsv\n",
    "#     24), max_display=display_count\n",
    "# )\n",
    "# plt.savefig('summary_plot_all_{}.png'.format(dataset_label), bbox_inches=\"tight\", dpi=600)\n",
    "# plt.savefig('summary_plot_all_{}.svg'.format(dataset_label), bbox_inches=\"tight\", dpi=600)\n",
    "# plt.savefig('summary_plot_all_{}.pdf'.format(dataset_label), bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display_count = 50\n",
    "# shap_values = explainer.shap_values(X = X.iloc[0:head_samples,:],\n",
    "#                                          nsamples=shap_nsamples)\n",
    "plt.title('Summary plot: Feature impact on day-ahead Load (kW) forecast ({} data, {} vars)'.format(dataset_label, display_count))\n",
    "shap.summary_plot(\n",
    "    shap_values, X, show=False, class_names=shap_legend,\n",
    "    color=pl.get_cmap(\"rainbow\", # rainbow, turbo, jet\n",
    "                      24), max_display=display_count)\n",
    "plt.savefig('summary_plot_all_{}_display{}.png'.format(dataset_label, display_count), bbox_inches=\"tight\", dpi=600)\n",
    "plt.savefig('summary_plot_all_{}_display{}.svg'.format(dataset_label, display_count), bbox_inches=\"tight\", dpi=600)\n",
    "plt.savefig('summary_plot_all_{}_display{}.pdf'.format(dataset_label, display_count), bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_label = \"test\"\n",
    "shap_values_test = explainer.shap_values(X = X_test.iloc[0:head_samples,:],\n",
    "                                         nsamples=shap_nsamples)\n",
    "plt.title('Summary plot: Feature impact on day-ahead Load (kW) forecast ({} data)'.format(dataset_label))\n",
    "shap.summary_plot(\n",
    "    shap_values_test, X_test, show=False, class_names=shap_legend,\n",
    "    color=pl.get_cmap(\"rainbow\", # rainbow, turbo, jet\n",
    "                      24),)\n",
    "plt.savefig('summary_plot_all_{}.png'.format(dataset_label), bbox_inches=\"tight\", dpi=600)\n",
    "plt.savefig('summary_plot_all_{}.svg'.format(dataset_label), bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display_count = 50\n",
    "dataset_label = \"test\"\n",
    "shap_values_test = explainer.shap_values(X = X_test.iloc[0:head_samples,:],\n",
    "                                         nsamples=shap_nsamples)\n",
    "plt.title('Summary plot: Feature impact on day-ahead Load (kW) forecast ({} data, {} vars)'.format(dataset_label, display_count))\n",
    "shap.summary_plot(\n",
    "    shap_values_test, X_test, show=False, class_names=shap_legend,\n",
    "    color=pl.get_cmap(\"rainbow\", # rainbow, turbo, jet\n",
    "                      24), max_display=display_count)\n",
    "plt.savefig('summary_plot_all_{}_display{}.png'.format(dataset_label, display_count), bbox_inches=\"tight\", dpi=600)\n",
    "plt.savefig('summary_plot_all_{}_display{}.svg'.format(dataset_label, display_count), bbox_inches=\"tight\", dpi=600)\n",
    "plt.savefig('summary_plot_all_{}_display{}.pdf'.format(dataset_label, display_count), bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_label = \"val\"\n",
    "shap_values_val = explainer.shap_values(X = X_val.iloc[0:head_samples,:],\n",
    "                                         nsamples=shap_nsamples)\n",
    "plt.title('Summary plot: Feature impact on day-ahead Load (kW) forecast ({} data)'.format(dataset_label))\n",
    "shap.summary_plot(\n",
    "    shap_values_val, X_val, show=False, class_names=shap_legend,\n",
    "    color=pl.get_cmap(\"rainbow\", # rainbow, turbo, jet\n",
    "                      24),)\n",
    "plt.savefig('summary_plot_all_{}.png'.format(dataset_label), bbox_inches=\"tight\", dpi=600)\n",
    "plt.savefig('summary_plot_all_{}.svg'.format(dataset_label), bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## TODO: Effect of specific input to specific output\n",
    "## AKA: explain individual INPUTs to specific OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Impact of last inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "last_idx = X.shape[0] - 1\n",
    "for hour_ahead in range(y.columns.shape[0]):\n",
    "    shap.initjs()\n",
    "    current_label = widgets.Dropdown(options=tuple_of_labels, value=hour_ahead, description='Select Label:')\n",
    "    plt.title('Force plot: Feature impact on last index {}h-ahead Load (kW) forecast (index: {}, date: {})'.format(hour_ahead + 1, last_idx, date_indices[last_idx]))\n",
    "    print(f'Current Label Shown: {list_of_labels[current_label.value]}\\n')\n",
    "\n",
    "    shap.force_plot(base_value = explainer.expected_value[current_label.value],\n",
    "                shap_values = shap_value_single[current_label.value],\n",
    "                features = X.iloc[last_idx:last_idx+1,:],\n",
    "                                   show=False,matplotlib=True\n",
    "                                   ).savefig('force_plots/force_plot-{}h-ahead_index{}.svg'.format(hour_ahead+1, last_idx), bbox_inches=\"tight\", dpi=600)\n",
    "\n",
    "    shap.force_plot(base_value = explainer.expected_value[current_label.value],\n",
    "                shap_values = shap_value_single[current_label.value],\n",
    "                features = X.iloc[last_idx:last_idx+1,:],\n",
    "                                   show=False,matplotlib=True\n",
    "                                   ).savefig('force_plots/force_plot-{}h-ahead_index{}.png'.format(hour_ahead+1, last_idx), bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Decision plot (see: https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/decision_plot.html)\n",
    "How models make decisions on a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Date should be index = end - (48 - hour_ahead)\n",
    "\n",
    "for hour_ahead in range(y.columns.shape[0]):\n",
    "    shap.initjs()\n",
    "    current_label = widgets.Dropdown(options=tuple_of_labels, value=hour_ahead, description='Select Label:')\n",
    "    plt.title('Decision plot: Feature impact on {}h-ahead Load (kW) forecast (index: {}, date: {})'.format(hour_ahead + 1, last_idx, date_indices[last_idx]))\n",
    "    fig1 = shap.decision_plot(base_value = explainer.expected_value[current_label.value],\n",
    "                shap_values = shap_value_single[current_label.value],\n",
    "                features = X.iloc[last_idx:last_idx+1,:],\n",
    "                              show=False\n",
    "                              )\n",
    "    plt.savefig('decision_plots/decision_plot-{}h-ahead_index{}.svg'.format(hour_ahead+1, last_idx), bbox_inches=\"tight\", dpi=600)\n",
    "    plt.savefig('decision_plots/decision_plot-{}h-ahead_index{}.png'.format(hour_ahead+1, last_idx), bbox_inches=\"tight\", dpi=600)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(X.iloc[last_idx:last_idx+1,:])\n",
    "print(y.iloc[last_idx:last_idx+1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# TODO: Keras tuner (see: https://www.tensorflow.org/tutorials/keras/keras_tuner?hl=uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize pixel values between 0 and 1\n",
    "img_train = img_train.astype('float32') / 255.0\n",
    "img_test = img_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "  # Tune the number of units in the first Dense layer: 32-512\n",
    "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "  model.add(keras.layers.Dense(10))\n",
    "\n",
    "  # Tune the learning rate for the optimizer: 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy', max_epochs=100,\n",
    "                     factor=3, directory='my_dir',\n",
    "                     project_name='intro_to_kt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=stop_early_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuner.search(img_train, label_train, epochs=100, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(img_train, label_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hypermodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Retrain the model\n",
    "hypermodel.fit(img_train, label_train, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_result = hypermodel.evaluate(img_test, label_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "time_series.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
